<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog de Tecnolog√≠a</title>
    <link rel="icon" href="../media/imagen41.png" type="image/png">
    <script src="https://kit.fontawesome.com/9474e300a6.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../css/estilos-articulo.css">
</head>
<body>

    <header>
        <div class="container__header">
            <div class="logo">
                <img src="../media/logo-1.png" alt="">
            </div>

            <div class="menu">
                <nav>
                    <ul>
                        <li><a href="../index.html">Inicio</a></li>
                        <li><a href="../electronica.html">Electr√≥nica</a></li>
                        <li><a href="../network.html">Networking</a></li>
                        <li><a href="../ia.html">Inteligencia Artificial</a></li>
                        <li><a href="../hacking.html">Ciberseguridad</a></li>
                        <li><a href="../devops.html">DevOps</a></li>
                    </ul>
                </nav>
            </div>
            <i class="fa-solid fa-bars" style="color: #ffffff;" id="icon_menu"></i>
            <div class="header__botonMenu">
                <input type="button" class="btn__header-botonMenu" value="Aportar" onclick="window.open('https://buymeacoffee.com/ryuzak1', '_blank');">
            </div>
        </div>
    </header>
    <main>
        <div class="cover">
            <div class="text__articulo-cover">
                <br>
                <br>
                <h1>Apache Spark</h1>
                <p>Apache Spark es un motor de procesamiento de datos en paralelo, dise√±ado espec√≠ficamente para grandes vol√∫menes de datos. Permite realizar c√°lculos distribuidos en un cl√∫ster de computadoras, ejecutando tareas de an√°lisis de datos, procesamiento en tiempo real y aprendizaje autom√°tico de manera r√°pida y escalable. Spark facilita la distribuci√≥n y paralelizaci√≥n de tareas para que puedan ejecutarse en m√∫ltiples nodos al mismo tiempo. Adem√°s, Spark es compatible con varios lenguajes de programaci√≥n, como Python (a trav√©s de PySpark), Scala, Java y R.</p>
                <div class="blog-image">
                    <img src="media/spark2.png" alt="">
                </div>
                <h2>Ubuntu</h2>
                <p>Ubuntu es un sistema operativo basado en Linux. Su funci√≥n principal es gestionar los recursos f√≠sicos de una m√°quina (como la CPU, la memoria y el almacenamiento), proporcionando una plataforma sobre la cual se ejecutan aplicaciones y servicios. Aunque Ubuntu en s√≠ no tiene ninguna funcionalidad espec√≠fica para el procesamiento de big data, es un sistema operativo muy utilizado en servidores y en cl√∫steres de computadoras que manejan grandes vol√∫menes de datos debido a su estabilidad, seguridad y flexibilidad.</p>
                <p>Es com√∫n instalar Apache Spark en servidores con Ubuntu. Esto permite que Spark aproveche las capacidades de Ubuntu para gestionar los recursos de hardware, mientras Spark maneja el procesamiento distribuido de los datos.</p>
                <p><marcador class="resaltado9">Escalabilidad: </marcador>Mientras que Ubuntu se limita a una sola m√°quina (o nodo), Spark permite utilizar m√∫ltiples m√°quinas al mismo tiempo (un cl√∫ster) para distribuir el procesamiento de datos. Esto es esencial en big data, donde el tama√±o de los datos es tan grande que una sola m√°quina no es suficiente.
                <p>En un entorno de big data, Ubuntu proporciona la base sobre la cual Spark se instala y ejecuta para distribuir y procesar datos masivos de forma r√°pida y eficiente.</p>
                <div class="blog-image-grande">
                    <img src="media/spark1.jpg" alt="">
                </div>
                <h2>Arquitectura</h2>
                <p>En Apache Spark (y en muchos otros sistemas distribuidos), el cl√∫ster se organiza en una arquitectura Master-Slave (Maestro-Esclavo), donde hay un nodo maestro y varios nodos trabajadores (workers). Esta estructura permite una distribuci√≥n y administraci√≥n eficiente del trabajo en el cl√∫ster.</p>
                <p>Apache Spark se divide en:</p>
                <ol style="list-style-type: none;">
                    <li>&#9658; <marcador class="resaltado3">Nodo Maestro (Master):</marcador></li>
                    <ul style="list-style-type: none;">
                        <li>üî∏El nodo maestro coordina y administra el cl√∫ster.</li>
                        <li>üî∏Asigna tareas a los nodos trabajadores y realiza un seguimiento de su progreso.</li>
                        <li>üî∏Maneja la divisi√≥n y distribuci√≥n de los trabajos (o "jobs") que Spark recibe para procesar.</li>
                        <li>üî∏Gestiona la divisi√≥n de tareas en subtareas m√°s peque√±as y la asignaci√≥n de estas a los nodos trabajadores.</li>
                    </ul>
                    <li>&#9658; <marcador class="resaltado3">Nodos Trabajadores (Workers):</marcador></li>
                    <ul style="list-style-type: none;">
                        <li>üî∏Los nodos trabajadores realizan el procesamiento real de los datos.</li>
                        <li>üî∏Cada worker tiene m√∫ltiples ejecutores (executors) que ejecutan las tareas espec√≠ficas asignadas por el nodo maestro.</li>
                        <li>üî∏Los workers reciben sus tareas del nodo maestro y ejecutan operaciones en los datos, como filtros, mapeos, agregaciones, etc.</li>
                        <li>üî∏Cada worker se encarga de una parte del conjunto de datos, proces√°ndolos en paralelo con otros workers, lo que permite manejar grandes vol√∫menes de datos de manera r√°pida.</li>
                    </ul>
                    <li>&#9658; <marcador class="resaltado3">Driver:</marcador></li>
                    <ul style="list-style-type: none;">
                        <li>üî∏El Driver es la aplicaci√≥n que env√≠a el trabajo al cl√∫ster. Inicia el proceso de c√°lculo en Spark, definiendo la l√≥gica del procesamiento y los datos que se van a analizar.</li>
                        <li>üî∏Interact√∫a directamente con el nodo maestro para crear un DAG (gr√°fico ac√≠clico dirigido) de tareas, que luego es gestionado por el maestro.</li>
                    </ul>
                    <li>&#9658; <marcador class="resaltado3">Cluster Manager:</marcador></li>
                    <ul style="list-style-type: none;">
                        <li>üî∏Apache Spark puede ejecutarse en varios sistemas de gesti√≥n de cl√∫steres, como YARN (en Hadoop), Mesos o Kubernetes.</li>
                        <li>üî∏El gestor del cl√∫ster asigna recursos a los nodos del cl√∫ster (como memoria y CPU), de modo que Spark pueda ejecutar tareas de manera eficiente.</li>
                    </ul>
                </ol>
                <h2>Flujo de trabajo</h2>
                <ol>
                    <li>El Driver inicia una aplicaci√≥n Spark, enviando el trabajo al nodo maestro.</li>
                    <li>El nodo maestro recibe el trabajo, lo descompone en tareas y las distribuye entre los nodos trabajadores.</li>
                    <li>Los workers ejecutan las tareas en paralelo, utilizando sus propios recursos de CPU y memoria.</li>
                    <li>Los resultados parciales de cada worker se env√≠an de regreso al maestro, que luego los combina para producir el resultado final.</li>
                </ol>
                <div class="blog-image-grande">
                    <img src="media/spark4.png" alt="">
                </div>
                <h2>Ventajas de la arquitectura Maestro-Trabajador:</h2>
                <ul>
                    <li><marcador class="resaltado9">Escalabilidad: </marcador>Permite procesar grandes cantidades de datos distribuyendo el trabajo entre varios nodos.</li>
                    <li><marcador class="resaltado9">Tolerancia a fallos: </marcador>Si un nodo trabajador falla, el maestro puede reasignar la tarea a otro nodo disponible.</li>
                    <li><marcador class="resaltado9">Flexibilidad: </marcador>Se puede usar con distintos sistemas de gesti√≥n de cl√∫steres y en la nube o en instalaciones locales.</li>
                </ul>
                <p>Entonces, s√≠, hay un nodo maestro en la orquestaci√≥n de cl√∫steres en Spark. Este nodo maestro coordina y organiza el procesamiento, mientras que los nodos trabajadores ejecutan las tareas distribuidas, permitiendo manejar datos a gran escala con eficiencia y rapidez.</p>
                <h2>Pandas vs PySpark</h2>
                <p>PySpark y pandas son similares en cuanto a su prop√≥sito, ya que ambas herramientas se utilizan para manipular y analizar datos. Sin embargo, tienen diferencias clave, especialmente en el tipo de datos que manejan y el tama√±o de los conjuntos de datos para los que est√°n dise√±ados.</p>
                <p>Las diferencias principales entre PySpark y pandas son:</p>
                <ol>
                    <li><marcador class="resaltado9">Escalabilidad y manejo de grandes vol√∫menes de datos: </marcador></li>
                    <ul style="list-style-type: none;">
                        <li>‚ñ± <marcador class="resaltado8">Pandas: </marcador>Su principal estructura de datos es el DataFrame, que es una tabla en memoria con filas y columnas. Pandas proporciona una gran cantidad de m√©todos y funciones para trabajar con datos en formato tabular de manera muy eficiente.</li>
                        <li>‚ñ± <marcador class="resaltado8">PySpark: </marcador>Tambi√©n utiliza DataFrames, pero estos son estructuras distribuidas que se procesan en paralelo en un cl√∫ster de Spark. Esto hace que algunos m√©todos y operaciones en PySpark sean diferentes o no tan directos como en pandas, ya que deben realizarse de forma distribuida.</li>
                    </ul>
                    <li><marcador class="resaltado9">Estructura de datos: </marcador></li>
                    <ul style="list-style-type: none;">
                        <li>‚ñ± <marcador class="resaltado8">Pandas: </marcador>Es muy r√°pido en operaciones en una sola m√°quina, especialmente para conjuntos de datos peque√±os y medianos.</li>
                        <li>‚ñ± <marcador class="resaltado8">PySpark: </marcador>Aunque tiene una latencia m√°s alta en la inicializaci√≥n y ciertas operaciones, debido a la coordinaci√≥n entre m√°quinas en el cl√∫ster, su ventaja es que puede procesar terabytes de datos en paralelo, lo que lo hace mucho m√°s r√°pido que pandas para grandes vol√∫menes.</li>
                    </ul>
                    <li><marcador class="resaltado9">Velocidad de procesamiento: </marcador></li>
                    <ul style="list-style-type: none;">
                        <li>‚ñ± <marcador class="resaltado8">Pandas: </marcador>Tiene una gran cantidad de funciones para manipular datos en memoria, y su sintaxis es bastante sencilla y directa.</li>
                        <li>‚ñ± <marcador class="resaltado8">PySpark: </marcador>Tiene una sintaxis algo similar a pandas, pero algunas operaciones pueden ser menos intuitivas debido a la necesidad de realizar procesamiento distribuido. Adem√°s, muchas funciones deben expresarse usando el API de Spark, que tiene sus propias particularidades.</li>
                    </ul>
                    <li><marcador class="resaltado9">Uso de funciones y operaciones: </marcador></li>
                    <ul style="list-style-type: none;">
                        <li>‚ñ± <marcador class="resaltado8">Pandas: </marcador>Para proyectos que pueden manejarse en una sola m√°quina y cuando el tama√±o de los datos es lo suficientemente peque√±o como para caber en la memoria RAM.</li>
                        <li>‚ñ± <marcador class="resaltado8">PySpark: </marcador>Cuando trabajas con big data, necesitas procesamiento en paralelo o est√°s trabajando en un entorno de cl√∫ster.</li>
                    </ul>
                </ol>
                <h6>Ejemplo de c√≥digo de comparaci√≥n</h6>
                <p>Una comparaci√≥n simple entre pandas y PySpark para una operaci√≥n com√∫n (filtrar datos):</p>
                <div class="archivo">
# Pandas
import pandas as pd
data = pd.DataFrame({'col1': [1, 2, 3, 4, 5], 'col2': [10, 20, 30, 40, 50]})
filtered_data = data[data['col1'] > 2]

# PySpark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Example").getOrCreate()
data = spark.createDataFrame([(1, 10), (2, 20), (3, 30), (4, 40), (5, 50)], ["col1", "col2"])
filtered_data = data.filter(data.col1 > 2)
                </div>
                <div class="blog-image-grande">
                    <img src="media/spark6.jpg" alt="">
                </div>
                <h2>PySpark</h2>
                <p>PySpark es una interfaz de Python para Apache Spark, un motor de procesamiento de datos de c√≥digo abierto dise√±ado para analizar grandes vol√∫menes de datos de manera r√°pida y eficiente. Spark permite distribuir y procesar datos en paralelo en varios nodos de un cl√∫ster, lo cual lo hace ideal para manejar grandes conjuntos de datos. PySpark facilita el uso de Spark desde Python, permitiendo a los desarrolladores manipular grandes cantidades de datos y realizar tareas de machine learning, an√°lisis de datos y ETL (extracci√≥n, transformaci√≥n y carga) con un enfoque similar a pandas.</p>
                <p>PySpark es especialmente √∫til cuando:</p>
                <ul>
                    <li>Tienes grandes vol√∫menes de datos que no caben en la memoria de una sola m√°quina.</li>
                    <li>Quieres realizar c√°lculos distribuidos de manera r√°pida y en paralelo.</li>
                    <li>Buscas integrar machine learning y an√°lisis de datos escalables en un cl√∫ster.</li>
                </ul>
                <p>Para usar PySpark en tu entorno local o en un cl√∫ster, necesitas instalar PySpark.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Instalaci√≥n con pip:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="pip install pyspark" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Una vez instalado, puedes iniciar PySpark en un entorno interactivo de Python (por ejemplo, Jupyter Notebook) o ejecutar scripts de Python que usen PySpark.</p>
                <p>‚ùè Aqu√≠ hay un ejemplo simple de c√≥mo iniciar un <marcador class="subrayado">SparkSession</marcador>, que es la entrada principal para trabajar con PySpark:</p>
                <div class="archivo">
from pyspark.sql import SparkSession

# Crear una SparkSession
spark = SparkSession.builder \
    .appName("MiAplicacionPySpark") \
    .getOrCreate()

# Crear un DataFrame de ejemplo
datos = [("Alice", 34), ("Bob", 45), ("Catherine", 29)]
columnas = ["Nombre", "Edad"]

df = spark.createDataFrame(datos, columnas)

# Mostrar el DataFrame
df.show()
                </div>
                <p>‚ùè PySpark permite cargar datos desde archivos CSV, JSON, Parquet y otras fuentes. Por ejemplo, para leer un archivo CSV:</p>
                <div class="archivo">
df = spark.read.csv("ruta/del/archivo.csv", header=True, inferSchema=True)
df.show()
                </div>
                <p>‚ùè Luego puedes usar m√©todos similares a los de pandas para manipular los datos:</p>
                <div class="archivo">
# Seleccionar columnas
df.select("Nombre").show()

# Filtrar datos
df.filter(df["Edad"] > 30).show()

# Agrupar y contar
df.groupBy("Edad").count().show()
                </div>
                <p>‚ùè PySpark tambi√©n tiene una biblioteca de machine learning llamada <marcador class="subrayado">MLlib</marcador>, que permite construir y entrenar modelos en grandes conjuntos de datos:</p>
                <div class="archivo">
from pyspark.ml.classification import LogisticRegression

# Preparar los datos para machine learning
# Definir el modelo
lr = LogisticRegression(featuresCol="caracter√≠sticas", labelCol="etiqueta")

# Entrenar el modelo
modelo = lr.fit(df)
                </div>
                <p>Para aprovechar al m√°ximo PySpark, generalmente se usa en un cl√∫ster de Spark. Esto se puede configurar en servicios en la nube (como AWS o Google Cloud) o en sistemas distribuidos como Hadoop.</p>
                <h2>Ejemplos</h2>
                <p>Haz click <a href="https://github.com/Detective-Ryuzak1/Machine-Learnig-examples/blob/main/Pyspark/PySpark.ipynb" target="blank">aqu√≠</a> para ver algunos ejemplos de uso con PySpark.</p>
                <p>Para m√°s ejemplos, visita la siguiente p√°gina con <a href="https://runawayhorse001.github.io/LearningApacheSpark/rdd.html#create-rdd" target="blank">documentaci√≥n</a> relevante.</p>
                <div class="blog-image-grande">
                    <img src="media/spark3.png" alt="">
                </div>
                <h2>Instalaci√≥n</h2>
                <p>Para configurar un cl√∫ster de Spark con un maestro y dos workers, hay algunos pasos adicionales adem√°s de simplemente instalar pyspark. La instalaci√≥n de pyspark te permitir√° ejecutar Spark en tu computadora, pero no configura un cl√∫ster completo. Aqu√≠ te explico los pasos detallados para lograrlo en tu distribuci√≥n de Linux:</p>
                <ul>
                    <li>Paso 1: Instalar Java</li>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Instalaci√≥n con apt:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="sudo apt install default-jdk" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Aseg√∫rate de tener Java instalado:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="java -version" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <li>Paso 2: Descargar e instalar Apache Spark</li>
                    <p>Ve a la p√°gina oficial de <a href="https://spark.apache.org/downloads.html" target="blank">Apache Spark</a> y descarga la versi√≥n m√°s reciente (o una versi√≥n compatible con PySpark y Jupyter).</p>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Extrae el archivo:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="tar -xvf spark-version-bin-hadoop-version.tgz" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Mueve la ubicaci√≥n del archivo:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="mv spark-1.3.1-bin-hadoop2.6 /usr/local/spark" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <p>Configura las variables de entorno para que puedas acceder a Spark desde cualquier terminal:</p>
                    <div class="archivo">
export PATH=$PATH:/usr/local/spark/bin
export PATH=$PATH:/usr/local/spark/sbin
source ~/.bashrc                     
                    </div>
                    <li>Paso 3: Instalar PySpark y Jupyter</li>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Ejecuta el siguiente comando para instalar PySpark y Jupyter en tu entorno de Python:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="pip install pyspark jupyter" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <li>Paso 4: Configurar el cl√∫ster de Spark</li>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Entra en los archivos de configuraci√≥n:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="cd spark-version-bin-hadoop-version/conf" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">
                            Crea una copia del script de bash:
                        </div>
                        <div class="comandos">
                          <input type="text" class="text" value="cp spark-env.sh.template spark-env.sh" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <p>Abre <marcador class="resaltado1">conf/spark-env.sh</marcador> con tu editor preferido (ej. nano) y agrega las siguientes l√≠neas para definir el nodo maestro y los workers:</p>
                    <div class="archivo">
# Establece el modo de ejecuci√≥n
export SPARK_MASTER_HOST='localhost' # Cambia esto si tu master no est√° en localhost
export SPARK_MASTER_PORT=7077        # Puerto donde el master escuchar√° las conexiones (predeterminado 7077)
export SPARK_WORKER_INSTANCES=2      # N√∫mero de workers en esta m√°quina
export SPARK_WORKER_CORES=2          # N√∫mero de n√∫cleos por trabajador
export SPARK_WORKER_MEMORY=4g        # Memoria para cada trabajador
export SPARK_DRIVER_MEMORY=4g        # Memoria para el driver                  
                    </div>
                    <p>En el archivo <marcador class="resaltado1">spark-env.sh</marcador> de la m√°quina que actuar√° como maestro, establece SPARK_MASTER_HOST con la IP del nodo maestro. Esto asegura que el maestro se identifique correctamente en la red.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Renombra el archivo <marcador class="resaltado1">conf/workers.template</marcador>:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="mv workers.template workers" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Abre <marcador class="resaltado1">workers</marcador> con tu editor y agrega las siguientes l√≠neas:</p>
                <div class="archivo">
# Nodo master (puedes incluir localhost si est√°s usando una sola m√°quina)
localhost

# Workers
worker1
worker2                
                </div>
                <p>Este archivo debe contener la lista de los nodos worker que participar√°n en el cl√∫ster. </p>
                <li>Paso 5: Iniciar el cl√∫ster</li>
                <div class="contenedor">
                    <div class="etiqueta">
                        Ejecuta el siguiente comando para iniciar el master:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="sbin/start-master.sh" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Esto iniciar√° el master de Spark. El master estar√° escuchando por defecto en http://localhost:8080, aunque puedes cambiar la direcci√≥n o el puerto si lo necesitas.</p>
                <p>Ahora, en cada uno de los nodos worker, necesitas iniciar los workers. Aseg√∫rate de que cada worker tenga acceso a la m√°quina master y que puedan comunicarse a trav√©s de la red.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        En cada worker, ejecuta el siguiente comando:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="sbin/start-worker.sh spark://localhost:7077" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Aqu√≠, el argumento spark://localhost:7077 especifica la direcci√≥n del master. Si tu master no est√° en localhost, debes poner la IP o nombre de host del master. Por ejemplo, si el master tiene la IP 192.168.1.100, el comando ser√≠a:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="sbin/start-worker.sh spark://localhost:7077" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Ahora puedes verificar el estado de tu cl√∫ster. Ve a la interfaz web del master en tu navegador:</p>
                <p>‚ñ± <marcador class="resaltado8">Interfaz web del master: </marcador>http://localhost:8080 (cambia localhost por la IP o nombre de host del master si est√° en una m√°quina remota).</p>
                <p>Si los workers est√°n bien conectados, deber√≠as ver el estado de los workers en la interfaz web.</p>
                <li>Paso 6: Ejecutar trabajos</li>
                <div class="contenedor">
                    <div class="etiqueta">
                        Una vez que el cl√∫ster est√© corriendo, puedes enviar trabajos a trav√©s de <marcador class="subrayado">spark-submit</marcador>. Si quieres enviar un trabajo al cl√∫ster de Spark, usa el siguiente comando, especificando la direcci√≥n del master:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="bin/spark-submit --class com.example.MyApp --master spark://localhost:7077 /path/to/your/spark-app.jar " oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <h6>Configurar el entorno de Jupyter para usar Spark</h6>
                <p>Para conectarte al cl√∫ster de Spark desde Jupyter, debes configurar las variables de entorno que Spark necesita para conectarse a tu master y workers. Hay varias maneras de hacerlo, pero una de las formas m√°s simples es exportando estas variables de entorno antes de iniciar Jupyter.</p>
                <p>Puedes establecer las variables de entorno necesarias para Spark directamente en la terminal o dentro de tu script de Jupyter. Estas variables indican a Spark que debe conectarse a tu cl√∫ster en lugar de ejecutarse en un modo local.</p>
                <p>En tu terminal (donde vayas a iniciar Jupyter), exporta las siguientes variables antes de iniciar Jupyter:</p>
                <div class="archivo">
export SPARK_HOME=/ruta/a/tu/spark-3.5.3-bin-hadoop3   # Ruta donde tienes instalado Spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
export PYSPARK_PYTHON=/usr/bin/python3        # Aseg√∫rate de que se use la versi√≥n correcta de Python
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --port=8888"
export SPARK_MASTER=spark://localhost:7077  # Direcci√≥n de tu master (puede ser otro nodo si no est√° en localhost)  
                </div>
                <p>Una vez que hayas exportado las variables de entorno, puedes iniciar Jupyter y conectarte al nodo master.</p>
                <div class="archivo">
import pyspark
sc = pyspark.SparkContext(master='spark://localhost:7077', appName='test')
                </div>
                <li>Paso 7: Detener el cl√∫ster</li>
                <p>Cuando hayas terminado, puedes detener el master y los workers con los siguientes comandos:</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Para detener el master:
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="sbin/stop-master.sh" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Para detener los workers (en cada worker):
                    </div>
                    <div class="comandos">
                      <input type="text" class="text" value="sbin/stop-worker.sh" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                </ul>
                <div class="blog-image-grande">
                    <img src="media/spark5.png" alt="">
                </div>
            </div>  
        </div>     
    </main>
    <script src="../js/script.js"></script>
</body>
</html>