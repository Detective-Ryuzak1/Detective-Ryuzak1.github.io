<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog de Tecnolog√≠a</title>
    <link rel="icon" href="../media/imagen41.png" type="image/png">
    <script src="https://kit.fontawesome.com/9474e300a6.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../css/estilos-articulo.css">
</head>
<body>

    <header>
        <div class="container__header">
            <div class="logo">
                <img src="../media/logo-1.png" alt="">
            </div>

            <div class="menu">
                <nav>
                    <ul>
                        <li><a href="../index.html">Inicio</a></li>
                        <li><a href="../electronica.html">Electr√≥nica</a></li>
                        <li><a href="../network.html">Networking</a></li>
                        <li><a href="../ia.html">Inteligencia Artificial</a></li>
                        <li><a href="../hacking.html">Ciberseguridad</a></li>
                        <li><a href="../devops.html">DevOps</a></li>
                    </ul>
                </nav>
            </div>
            <i class="fa-solid fa-bars" style="color: #ffffff;" id="icon_menu"></i>
            <div class="header__botonMenu">
                <input type="button" class="btn__header-botonMenu" value="Aportar" onclick="window.open('https://buymeacoffee.com/ryuzak1', '_blank');">
            </div>
        </div>
    </header>
    <main>
        <div class="cover">
            <div class="text__articulo-cover">
                <br>
                <br>
                <h1>LLM (Large Language Model)</h1>
                <p>Los Modelos de Lenguaje de Gran Escala (LLM, por sus siglas en ingl√©s) son redes neuronales entrenadas en grandes cantidades de datos textuales para comprender y generar lenguaje humano. Estos modelos utilizan arquitecturas como Transformers, popularizadas por el modelo GPT (Generative Pre-trained Transformer). Su funcionamiento se basa en el uso de millones o incluso miles de millones de par√°metros que ajustan las probabilidades de las palabras en una secuencia, permitiendo respuestas coherentes y contextualmente relevantes.</p>
                <div class="blog-image-grande">
                    <img src="media/llm1.png" alt="">
                </div>
                <h2>Modelos de C√≥digo Abierto</h2>
                <p>A diferencia de los modelos propietarios como ChatGPT de OpenAI o Gemini de Google, existen modelos de c√≥digo abierto que ofrecen flexibilidad y control para empresas y desarrolladores. Algunos de los m√°s populares incluyen:</p>
                <ul>
                    <li><marcador class="resaltado8">LLaMA (Meta): </marcador>Un modelo eficiente que puede ejecutarse en hardware de consumo.</li>
                    <li><marcador class="resaltado8">DeepSeek: </marcador>Un modelo de c√≥digo abierto optimizado para generaci√≥n de lenguaje y tareas avanzadas de IA, con enfoque en eficiencia y escalabilidad.</li>
                    <li><marcador class="resaltado8">Mistral: </marcador>Un modelo ligero y eficiente con alto rendimiento en tareas de lenguaje.</li>
                    <li><marcador class="resaltado8">Falcon (Technology Innovation Institute): </marcador>Optimizado para implementaciones locales.</li>
                    <li><marcador class="resaltado8">Bloom (BigScience): </marcador>Un modelo multiling√ºe entrenado en diversas fuentes de datos.</li>
                    <li><marcador class="resaltado8">GPT-NeoX y GPT-J (EleutherAI): </marcador>Alternativas abiertas a GPT-3 con buen rendimiento y optimizaci√≥n para tareas de lenguaje y programaci√≥n.</li>
                </ul>
                <p>Estos modelos pueden ejecutarse localmente, brindando independencia de servicios en la nube y mayor privacidad en el procesamiento de datos sensibles.</p>
                <div class="blog-image-grande">
                    <img src="media/llm2.png" alt="">
                </div>
                <h2>Implementaci√≥n Local de LLMs</h2>
                <p>Ejecutar un LLM de manera local requiere hardware potente, idealmente con GPUs o TPUs optimizadas para c√°lculo de redes neuronales. Herramientas como Ollama, LMStudio o llama.cpp permiten ejecutar modelos en equipos personales, utilizando t√©cnicas de cuantizaci√≥n para reducir la carga computacional.</p>
                <p>Para la inferencia local, se pueden utilizar frameworks como:</p>
                <ul>
                    <li><marcador class="resaltado8">Transformers (Hugging Face): </marcador>Facilita la carga y uso de modelos preentrenados.</li>
                    <li><marcador class="resaltado8">TensorRT y ONNX Runtime: </marcador>Optimizan el rendimiento en GPUs de NVIDIA.</li>
                    <li><marcador class="resaltado8">GGML/GGUF: </marcador>Permiten ejecutar modelos optimizados en CPUs.</li>
                </ul>
                <h2>Uso Profesional</h2>
                <p>Los usuarios pueden emplear LLMs para acceder a bases de datos personales y generar respuestas basadas en informaci√≥n propia. Para lograr esto, se emplean t√©cnicas como Retrieval-Augmented Generation (RAG), que combina generaci√≥n de texto con recuperaci√≥n de documentos relevantes desde una fuente de datos.</p>
                <h2>M√©todos de Conexi√≥n y APIs</h2>
                <p>Existen varias formas de conectar un LLM con bases de datos y sistemas empresariales:</p>
                <ul style="list-style-type: none;">
                    <li>ñ¶π <marcador class="resaltado4">APIs REST y WebSockets: </marcador>Se exponen endpoints para consultas en lenguaje natural. Ejemplo: Integraci√≥n con Elasticsearch o bases de datos SQL/NoSQL.</li>
                    <li>ñ¶π <marcador class="resaltado4">Conectores Directos a Bases de Datos: </marcador>Se usa un middleware para ejecutar consultas SQL. Integraci√≥n con herramientas como LangChain o LlamaIndex.</li>
                    <li>ñ¶π <marcador class="resaltado4">Vector Databases (Bases de Datos Vectoriales): </marcador>Permiten almacenar embeddings sem√°nticos generados por los LLMs. Ejemplo: Pinecone, Weaviate, ChromaDB, FAISS.</li>
                    <li>ñ¶π <marcador class="resaltado4">Plugins y Extensiones: </marcador>Empresas crean plugins para interactuar con CRMs, ERPs y otros sistemas corporativos.</li>
                </ul>
                <h2>Hugging Face</h2>
                <p>Hugging Face es una plataforma que centraliza modelos de IA, incluyendo LLMs, y proporciona herramientas para su implementaci√≥n. En su repositorio, se encuentran modelos de c√≥digo abierto para NLP, visi√≥n por computadora y m√°s.</p>
                <p>Tipos de Modelos en Hugging Face:</p>
                <ul>
                    <li><marcador class="resaltado8">Modelos de Lenguaje: </marcador>GPT, LLaMA, Mistral, DeepSeek, entre otros.</li>
                    <li><marcador class="resaltado8">Modelos de C√≥digo: </marcador>StarCoder, CodeLlama, SantaCoder.</li>
                    <li><marcador class="resaltado8">Modelos Multimodales: </marcador>CLIP, Flamingo, Whisper.</li>
                </ul>
                <h6>Descarga e Implementaci√≥n</h6>
                <ol>
                    <li>Instalar los paquetes:</li>
                    <div class="contenedor">
                        <div class="etiqueta">Crea el entrono virtual para aislar las dependencias del proyecto y evitar conflictos con otras instalaciones.</div>
                        <div class="comandos">
                          <input type="text" class="text" value="python -m venv venv" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">Activa el entorno virtual en Linux.</div>
                        <div class="comandos">
                          <input type="text" class="text" value="source .\venv\bin\activate" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">Activa el entorno virtual en Windows.</div>
                        <div class="comandos">
                          <input type="text" class="text" value=".\venv\Scripts\activate" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <div class="contenedor">
                        <div class="etiqueta">Instala los paquetes.</div>
                        <div class="comandos">
                          <input type="text" class="text" value="pip install transformers huggingface_hub" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <li>Crear un token de autenticaci√≥n en Hugging Face: Para hacerlo, accede a tu cuenta de Hugging Face, dir√≠gete a la secci√≥n de configuraci√≥n, luego a la pesta√±a "Access Tokens" y selecciona "New token". Elige un nombre y nivel de acceso adecuado, luego genera y copia el token para usarlo en futuras autenticaciones.</li>
                    <li>Iniciar sesi√≥n con Hugging Face CLI:</li>
                    <div class="contenedor">
                        <div class="etiqueta"></div>
                        <div class="comandos">
                          <input type="text" class="text" value="huggingface-cli login" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <li>Para descargar modelos desde Hugging Face, se usa la librer√≠a transformers de Python:</li>
                    <div class="archivo">from transformers import pipeline

model = pipeline(task:"summarization", model="facebook/bart-large-cnn")
response = model("text to summarize")
print(response)</div>
                </ol>
                <p>Esto permite cargar modelos listos para inferencia o fine-tuning en aplicaciones personalizadas.</p>
                <div class="blog-image-grande">
                    <img src="media/llm3.jpg" alt="">
                </div>
                <p>Puedes buscar modelos de forma manual en Hugging Face a trav√©s de su sitio web, luego hacer clic en la pesta√±a Models y si necesitas filtrarlo por tipo de modelo o framework, ir a la secci√≥n Libraries.</p>
                <p>En la secci√≥n Tasks de Hugging Face, puedes buscar modelos seg√∫n la tarea espec√≠fica que necesitas resolver. Al hacer clic en Tasks, ver√°s categor√≠as como:</p>
                <ul>
                    <li><marcador class="resaltado8">Text Generation: </marcador>Para generaci√≥n de texto (ej. GPT, LLaMA).</li>
                    <li><marcador class="resaltado8">Text Classification: </marcador>Para clasificar texto (ej. sentimiento, spam).</li>
                    <li><marcador class="resaltado8">Summarization: </marcador>Para resumir textos (ej. BART, T5).</li>
                    <li><marcador class="resaltado8">Translation: </marcador>Para traducci√≥n autom√°tica.</li>
                    <li><marcador class="resaltado8">Image Generation: </marcador>Para generar im√°genes (ej. Stable Diffusion).</li>
                    <li><marcador class="resaltado8">Object Detection: </marcador>Para detecci√≥n de objetos en im√°genes.</li>
                    <li><marcador class="resaltado8">Speech Recognition: </marcador>Para convertir audio en texto (ej. Whisper).</li>
                </ul>
                <p>Al seleccionar una tarea, <a href="https://huggingface.co/models" target="blank">Hugging Face</a> te mostrar√° los modelos m√°s populares y recomendados para esa funci√≥n. As√≠ puedes filtrar y elegir el mejor modelo sin necesidad de conocer su nombre exacto.</p>
                <h2>Tareas aceleradas por GPU</h2>
                <p>CUDA Toolkit es un conjunto de herramientas desarrollado por NVIDIA que permite a los desarrolladores aprovechar la potencia de las GPUs para ejecutar c√°lculos paralelos de alto rendimiento. Incluye compiladores, bibliotecas, y herramientas de depuraci√≥n para desarrollar aplicaciones en CUDA (Compute Unified Device Architecture), un modelo de programaci√≥n dise√±ado espec√≠ficamente para procesadores gr√°ficos de NVIDIA.</p>
                <p>Funciones:</p>
                <ul>
                    <li>Acelerar c√≥mputo paralelo en tareas como aprendizaje profundo, simulaciones cient√≠ficas y procesamiento de im√°genes.</li>
                    <li>Optimizar modelos de IA y Machine Learning, facilitando la ejecuci√≥n de frameworks como PyTorch y TensorFlow con soporte para GPU.</li>
                    <li>Mejorar el rendimiento de c√°lculos matem√°ticos intensivos mediante bibliotecas como cuBLAS (√°lgebra lineal), cuDNN (redes neuronales) y Thrust (programaci√≥n paralela en C++).</li>
                    <li>Facilitar la programaci√≥n en GPU a trav√©s de un entorno que incluye NVCC (compilador CUDA), CUDA Runtime API y herramientas de profiling como Nsight.</li>
                </ul>
                <p>Puedes obtener CUDA Toolkit directamente desde la <a href="https://developer.nvidia.com/cuda-downloads" target="blank">p√°gina oficial</a> de NVIDIA.</p>
                <h6>Pasos para usar CUDA</h6>
                <ol>
                    <li>Reiniciar la terminal</li>
                    <p>Si usas un entorno virtual como Conda o venv, act√≠valo.</p>
                    <li>Verificar la instalaci√≥n de CUDA</li>
                    <div class="contenedor">
                        <div class="etiqueta">Ejecuta el siguiente comando para asegurarte de que CUDA est√° instalado correctamente:</div>
                        <div class="comandos">
                          <input type="text" class="text" value="nvcc --version" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <li>Instalar PyTorch con la versi√≥n de CUDA detectada</li>
                    <div class="contenedor">
                        <div class="etiqueta">Basado en la versi√≥n de CUDA que obtuviste, instala PyTorch, torchvision y torchaudio con el √≠ndice correcto:</div>
                        <div class="comandos">
                          <input type="text" class="text" value="pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128" oninput="ajustarAncho(this)">
                          <button><i class="fa fa-clone"></i><span> Copy</span></button>
                        </div>
                    </div>
                    <p>(Si usas otra versi√≥n, cambia cu128 por la que corresponda, como cu121 para CUDA 12.1).</p>
                    <li>Verificar que PyTorch reconoce CUDA</li>
                    <p>Ejecuta el siguiente script en Python:</p>
                    <div class="archivo">import torch

# Verifica si CUDA est√° disponible
if torch.cuda.is_available():
    print("CUDA est√° disponible")
    print(f"GPU utilizada: {torch.cuda.get_device_name(0)}")
else:
    print("CUDA no est√° disponible")</div>
                    <li>Ejemplo de uso en Python con CUDA</li>
                    <p>Aqu√≠ se hace uso del script previo para usar CUDA y cambiar entre CPU y GPU din√°micamente:</p>
                </ol>
                <div class="archivo">import torch
from transformers import pipeline

# Determinar el dispositivo disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Usando: {device}")

# Cargar el modelo en el dispositivo seleccionado
model = pipeline(task="summarization", model="facebook/bart-large-cnn", device=0 if device == "cuda" else -1)

# Texto de prueba
text = """ New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.
A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.
Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes only within two weeks of each other.
In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her "first and only" marriage.
Barrientos, now 39, is facing two criminal counts of "offering a false instrument for filing in the first degree," referring to her false statements on the
2010 marriage license application, according to court documents.
Prosecutors said the marriages were part of an immigration scam.
On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.
After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective
Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.
All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.
Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.
Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
The case was referred to the Bronx District Attorney\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\'s
Investigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.
Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.
If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.
"""

# Generar el resumen
response = modelo(text)
print(response, max_length=130, min_length=30, do_sample=False)                   
                </div>
                <p>La librer√≠a transformers de Hugging Face permite cambiar entre CPU y GPU mediante el argumento device al crear el pipeline.</p>
                <p>Si tienes una GPU compatible con CUDA, el dispositivo ser√° "cuda", y debes pasar device=0 para indicar que se use la primera GPU disponible.</p>
                <p>Si no tienes una GPU compatible o deseas ejecutar el modelo en la CPU, el dispositivo ser√° "cpu", y debes pasar device=-1.</p>
                <div class="blog-image-grande">
                    <img src="media/llm4.png" alt="">
                </div>
                <h2>Langchain</h2>
                <p>LangChain es un marco de trabajo dise√±ado para desarrollar aplicaciones que utilizan modelos de lenguaje de manera m√°s estructurada y eficiente. Facilita la integraci√≥n de modelos de IA con bases de datos, APIs externas y herramientas avanzadas como recuperaci√≥n de informaci√≥n (RAG), memoria de contexto y toma de decisiones.</p>
                <p>Este framework es especialmente √∫til para construir chatbots, agentes aut√≥nomos, generaci√≥n de texto basada en datos, automatizaci√≥n de tareas con IA y muchas otras aplicaciones. Se puede usar con m√∫ltiples modelos de lenguaje como GPT-4, Mistral, LLaMA, Claude, entre otros.</p>
                <p>Principales funcionalidades:</p>
                <ol>
                    <li><marcador class="resaltado4">Prompts Personalizados: </marcador>LangChain permite definir plantillas de prompts para estructurar mejor las consultas que se env√≠an a los modelos, optimizando su rendimiento y obteniendo respuestas m√°s precisas.</li>
                    <p>Ejemplo:</p>
                    <div class="archivo">from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["producto"],
    template="Describe las caracter√≠sticas principales del producto {producto} en menos de 50 palabras."
)

print(prompt.format(producto="tel√©fono inteligente"))
                    </div>
                    <li><marcador class="resaltado4">Memoria (Contexto de Conversaci√≥n): </marcador>Permite a los modelos recordar interacciones previas dentro de una conversaci√≥n, lo que es clave para chatbots y asistentes de IA.</li>
                    <ul style="list-style-type: none;">
                        <li>‚å≠ <marcador class="resaltado8">Short-term memory: </marcador>Recuerda solo las √∫ltimas interacciones.</li>
                        <li>‚å≠ <marcador class="resaltado8">Long-term memory: </marcador>Almacena conversaciones m√°s largas, generalmente en bases de datos o archivos.</li>
                    </ul>
                    <p>Ejemplo de memoria en un chatbot:</p>
                    <div class="archivo">from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()
memory.save_context({"input": "Hola, ¬øc√≥mo est√°s?"}, {"output": "¬°Hola! Estoy bien, ¬øy t√∫?"})

print(memory.load_memory_variables({}))
                    </div>
                    <li><marcador class="resaltado4">Cadenas de Procesamiento (Chains): </marcador>Permite combinar m√∫ltiples pasos en una √∫nica ejecuci√≥n. Por ejemplo, recibir una consulta, reformatearla, obtener una respuesta de un modelo y devolver un resultado estructurado.</li>
                    <p>Ejemplo:</p>
                    <div class="archivo">from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

llm = OpenAI(model_name="gpt-4")
prompt = PromptTemplate(template="¬øQu√© sabes sobre {tema}?", input_variables=["tema"])
chain = LLMChain(llm=llm, prompt=prompt)

print(chain.run("inteligencia artificial"))
                    </div>
                    <li><marcador class="resaltado4">Conectores a Bases de Datos (Vector Stores): </marcador>LangChain permite usar bases de datos vectoriales como ChromaDB, FAISS, Weaviate, Pinecone, etc., para almacenar informaci√≥n en forma de embeddings y mejorar la b√∫squeda de datos en grandes vol√∫menes de texto.</li>
                    <p>Ejemplo con ChromaDB:</p>
                    <div class="archivo">from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

embeddings = OpenAIEmbeddings()
vectorstore = Chroma(embedding_function=embeddings)
                        </div>
                    <li><marcador class="resaltado4">Agentes y Herramientas: </marcador>LangChain permite crear agentes que pueden interactuar con m√∫ltiples herramientas y realizar tareas como consultas en bases de datos, llamadas a APIs, b√∫squedas web, c√°lculos matem√°ticos, etc.</li>
                    <p>Ejemplo de agente:</p>
                    <div class="archivo">from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool
from langchain.llms import OpenAI

def buscar_wikipedia(consulta):
    import wikipedia
    return wikipedia.summary(consulta, sentences=2)

herramientas = [
    Tool(name="Wikipedia", func=buscar_wikipedia, description="Busca en Wikipedia")
]

llm = OpenAI(model_name="gpt-4")
agente = initialize_agent(herramientas, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

print(agente.run("¬øQui√©n fue Nikola Tesla?"))    
                    </div>
                </ol>
                <h6>Ejemplo 1</h6>
                <div class="contenedor">
                    <div class="etiqueta">Es necesario instalar los paquetes antes de comenzar.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="pip install transformers langchain langchain-huggingface" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>El siguiente script en Python utiliza procesamiento de lenguaje natural (NLP) para resumir textos de manera inteligente y adaptada a la comprensi√≥n de diferentes grupos de edad.</p>
                <div class="archivo">from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers.utils.logging import set_verbosity_error
set_verbosity_error()

# Cargamos el modelo de resumen de Hugging Face
model = pipeline("summarization", model="facebook/bart-large-cnn", device=0)

# Envolvemos el modelo dentro de LangChain
# Esto nos permite integrarlo con otras funciones avanzadas de LangChain, como pipelines, agentes, etc.
llm = HuggingFacePipeline(pipeline=model)

# Creamos una plantilla de prompt con LangChain
# Esta plantilla personaliza el prompt para que el resumen sea comprensible seg√∫n la edad del usuario
template = PromptTemplate.from_template(
    "Summarize the following text in a way a {age} year old would understand:\n\n{text}"
)

# Combinamos la plantilla con el modelo de resumen
# Esto crea una "cadena" de procesamiento en LangChain que toma la entrada, la transforma con el prompt y la env√≠a al modelo
summarizer_chain = template | llm

# Solicitamos al usuario el texto que desea resumir y la edad objetivo para ajustar el nivel de simplificaci√≥n
text_to_summarize = input("\nEnter text to summarize:\n")
age = input("Enter target age for simplification:\n")

# Ejecutamos la cadena de procesamiento con los datos ingresados por el usuario
summary = summarizer_chain.invoke({"text": text_to_summarize, "age": age})

# Mostramos el resumen generado en la consola
print("\nüîπ **Generated Summary:**")
print(summary)
                </div>
                <h6>Ejemplo 2</h6>
                <p>Este script es una herramienta avanzada que combina la generaci√≥n de res√∫menes con la capacidad de responder preguntas sobre el contenido resumido. Utiliza modelos preentrenados de Hugging Face y la integraci√≥n con LangChain para ofrecer una experiencia completa de procesamiento de lenguaje natural (NLP).</p>
                <div class="archivo">from transformers import pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain.prompts import PromptTemplate

# Desactivamos los mensajes de advertencia en la consola para una salida m√°s limpia
from transformers.utils.logging import set_verbosity_error
set_verbosity_error()

# Cargamos el modelo de resumen inicial (facebook/bart-large-cnn)
# Este modelo es especializado en generar res√∫menes concisos.
summarization_pipeline = pipeline("summarization", model="facebook/bart-large-cnn", device=0)

# Envolvemos el modelo de resumen en LangChain para integrarlo con otras funcionalidades
summarizer = HuggingFacePipeline(pipeline=summarization_pipeline)

# Cargamos un segundo modelo de refinamiento para mejorar la calidad del resultado final
refinement_pipeline = pipeline("summarization", model="facebook/bart-large", device=0)

# Envolvemos el modelo de refinamiento en LangChain
refiner = HuggingFacePipeline(pipeline=refinement_pipeline)

# Cargamos el modelo de pregunta-respuesta (deepset/roberta-base-squad2)
# Este modelo es especializado en responder preguntas basadas en un contexto dado.
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2", device=0)

# Creamos una plantilla de prompt para personalizar la longitud del resumen
# El usuario puede elegir entre "short", "medium" o "long".
summary_template = PromptTemplate.from_template(
    "Summarize the following text in a {length} way:\n\n{text}"
)

# Combinamos la plantilla con los modelos de resumen y refinamiento (cadena de procesamiento en LangChain)
# Esto crea una cadena de procesamiento que genera y refina el resumen.
summarization_chain = summary_template | summarizer | refiner

# Solicitamos al usuario el texto que desea resumir
text_to_summarize = input("\nEnter text to summarize:\n")

# Pedimos al usuario que elija la longitud del resumen (corto, mediano o largo)
length = input("\nEnter the length (short/medium/long): ")

# Ejecutamos la cadena de procesamiento para generar el resumen
summary = summarization_chain.invoke({"text": text_to_summarize, "length": length})

# Mostramos el resumen generado en la consola
print("\nüîπ **Generated Summary:**")
print(summary)

# Bucle para hacer preguntas sobre el resumen
while True:
    # Solicitamos una pregunta al usuario
    question = input("\nAsk a question about the summary (or type 'exit' to stop):\n")
    
    # Si el usuario escribe "exit", salimos del bucle
    if question.lower() == "exit":
        break

    # Ejecutamos el modelo de pregunta-respuesta
    # El modelo toma la pregunta y el resumen como contexto para generar una respuesta.
    qa_result = qa_pipeline(question=question, context=summary)

    print("\nüîπ **Answer:**")
    print(qa_result["answer"])
                </div>
                <h6>Ejemplo 3</h6>
                <p>Para el siguiente ejemplo, debes instalar Ollama en tu sistema y luego utilizarlo desde la terminal o en Python. Aqu√≠ tienes los pasos detallados:</p>
                <p>Ollama est√° disponible para Windows, macOS y Linux. Desc√°rgalo desde la <a href="https://ollama.com/download" target="blank">web oficial</a></p>
                <div class="contenedor">
                    <div class="etiqueta">Verifica que Ollama est√° instalado.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="ollama --help" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">Ollama permite descargar modelos listos para usar. Por ejemplo, para descargar Llama 3.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="ollama pull llama3" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Puedes ver la lista completa en <a href="https://ollama.com/library" target="blank">Ollama Models</a>.</p>
                <div class="contenedor">
                    <div class="etiqueta">Una vez descargado un modelo, puedes interactuar con √©l desde la terminal.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="ollama run llama3" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">Si quieres usar Ollama en un script de Python, es necesario instalar las bibliotecas.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="pip install langchain langchain-ollama ollama" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Ejemplo de uso en Python:</p>
                <div class="archivo">from langchain_ollama import OllamaLLM

model = OllamaLLM(model="llama3")

result = model.invoke(input="¬øQu√© es la inteligencia artificial?")
print(result)
                </div>
                <p>El siguiente script implementa un chatbot interactivo utilizando un modelo de lenguaje local a trav√©s de Ollama y el framework LangChain. El chatbot es capaz de mantener una conversaci√≥n con el usuario, recordando el historial de la conversaci√≥n para proporcionar respuestas contextualmente relevantes.</p>
                <div class="archivo">from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = """
Answer the question below.

Here is the conversation history: {context}

Question: {question}

Answer:
"""

model = OllamaLLM(model="llama3")
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

def handle_conversation():
    context = ""
    print("Welcome to the AI Chatbot! Type 'exit' to quit.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break
        result = model.invoke({"context": context, "question": user_input})
        print("Bot: ", result)
        context += f"\nUser: {user_input}\nAI: {result}"

if __name__ == "__main__":
    handle_conversation()
                </div>
                <div class="blog-image-grande">
                    <img src="media/llm5.png" alt="">
                </div>
                <h2>Langflow</h2>
                <p>Langflow es una herramienta de desarrollo de interfaces gr√°ficas para aplicaciones de inteligencia artificial (IA), dise√±ada para facilitar la creaci√≥n, gesti√≥n y visualizaci√≥n de flujos de trabajo que involucran modelos de lenguaje. Se basa en LangChain, una librer√≠a popular en Python para construir aplicaciones con modelos de lenguaje como GPT, Mistral y otros.</p>
                <p>Langflow proporciona una interfaz visual intuitiva que permite a los desarrolladores y cient√≠ficos de datos construir cadenas de procesamiento de texto sin necesidad de programar directamente en Python, aunque tambi√©n permite integraciones avanzadas para usuarios m√°s experimentados.</p>
                <p>Langflow es una herramienta dise√±ada principalmente para facilitar el dise√±o y la implementaci√≥n de flujos de trabajo de inteligencia artificial (IA) de manera visual, eliminando la necesidad de escribir c√≥digo manualmente. Permite a los usuarios integrar y probar modelos de lenguaje con gran facilidad, lo que acelera el proceso de desarrollo y experimentaci√≥n. Adem√°s, Langflow es ideal para crear y compartir prototipos r√°pidamente, fomentando la colaboraci√≥n entre desarrolladores y equipos de IA. Tambi√©n ofrece la capacidad de optimizar flujos de procesamiento de texto, permitiendo ajustes r√°pidos y pruebas iterativas para mejorar la eficiencia y precisi√≥n de los sistemas de IA</p>
                <p>Langflow es altamente flexible y se puede integrar con diversas herramientas y plataformas:</p>
                <ul style="list-style-type: none;">
                    <li>‚óò <marcador class="resaltado4">Modelos de Lenguaje: </marcador>GPT-4, Mistral, LLaMA, PaLM, entre otros.</li>
                    <li>‚óò <marcador class="resaltado4">Bases de Datos: </marcador>PostgreSQL, MongoDB, Redis, entre otros.</li>
                    <li>‚óò <marcador class="resaltado4">APIs y Servicios Cloud: </marcador>OpenAI API, Hugging Face, Google Cloud AI, AWS SageMaker.</li>
                    <li>‚óò <marcador class="resaltado4">Frameworks de Machine Learning: </marcador>TensorFlow, PyTorch, Scikit-learn.</li>
                    <li>‚óò <marcador class="resaltado4">Aplicaciones Web y Backend: </marcador>FastAPI, Flask, Django, Streamlit.</li>
                </ul>
                <h6>Ejemplo</h6>
                <p>A continuaci√≥n, te explico c√≥mo puedes usar Langflow paso a paso:</p>
                <p></p>
                <div class="contenedor">
                    <div class="etiqueta">Antes de usar Langflow, debes instalarlo en tu entorno local. Puedes hacerlo f√°cilmente usando pip, el gestor de paquetes de Python.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="pip install langflow" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">Una vez instalado, puedes iniciar Langflow como un m√≥dulo de Python.</div>
                    <div class="comandos">
                      <input type="text" class="text" value="python -m langflow run" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Esto iniciar√° un servidor local.</p>
                <div class="contenedor">
                    <div class="etiqueta">Abre tu navegador web y visita la siguiente URL:</div>
                    <div class="comandos">
                      <input type="text" class="text" value="http://localhost:7860" oninput="ajustarAncho(this)">
                      <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Langflow ofrece una interfaz visual donde puedes arrastrar y soltar componentes para construir flujos de trabajo de IA.</p>
                <p>Componentes principales de Langflow:</p>
                <ul>
                    <li><marcador class="resaltado8">Editor Visual: </marcador>Interfaz gr√°fica de arrastrar y soltar para construir flujos de trabajo de IA.</li>
                    <li><marcador class="resaltado8">Nodos y Conectores: </marcador>Representan componentes como modelos de lenguaje, preprocesamiento de datos y conectores API.</li>
                    <li><marcador class="resaltado8">Biblioteca de Integraciones: </marcador>Soporte para servicios de terceros y modelos personalizados.</li>
                    <li><marcador class="resaltado8">Gestor de Variables y Par√°metros: </marcador>Permite modificar configuraciones sin cambiar la estructura del flujo.</li>
                    <li><marcador class="resaltado8">Compatibilidad con LangChain: </marcador>Facilita el uso de cadenas de procesamiento con capacidades avanzadas.</li>
                </ul>
                <p>Langflow permite exportar e importar modelos en formato JSON, lo que facilita su implementaci√≥n y reutilizaci√≥n en diferentes entornos. Para importar un modelo previamente guardado:</p>
                <ol>
                    <li>Accede a la interfaz de Langflow.</li>
                    <li>Para importar un modelo en Langflow, primero crea un nuevo flujo haciendo clic en "New Flow" en la interfaz principal. Una vez que tengas tu flujo en blanco, dir√≠gete a la parte superior de la pantalla donde dice "Untitled document" (o el nombre actual del flujo), haz clic para desplegar el men√∫ y selecciona la opci√≥n "Import".</li>
                    <li>Carga el archivo JSON del modelo previamente guardado.</li>
                    <li>Ajusta los par√°metros si es necesario y ejecuta el flujo.</li>
                </ol>
                <p>Tambien es posible integrar Langflow en un entorno de desarrollo usando LangChain, cargando modelos y flujos en Python para personalizaci√≥n avanzada.</p>
                <p>Haz click <a href="https://github.com/hunter-meloche/REMO-langflow" target="blank">aqu√≠</a> para ver un ejemplo de uso en LangChain.</p>
                <div class="blog-image-grande">
                    <img src="media/llm6.png" alt="">
                </div>
            </div>  
        </div>     
    </main>
    <script src="../js/script.js"></script>
</body>
</html>