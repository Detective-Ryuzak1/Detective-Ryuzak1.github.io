<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog de Tecnolog√≠a</title>
    <link rel="icon" href="../media/imagen41.png" type="image/png">
    <script src="https://kit.fontawesome.com/9474e300a6.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../css/estilos-articulo.css">
</head>
<body>

    <header>
        <div class="container__header">
            <div class="logo">
                <img src="../media/logo-1.png" alt="">
            </div>

            <div class="menu">
                <nav>
                    <ul>
                        <li><a href="../index.html">Inicio</a></li>
                        <li><a href="../electronica.html">Electr√≥nica</a></li>
                        <li><a href="../network.html">Networking</a></li>
                        <li><a href="../ia.html">Inteligencia Artificial</a></li>
                        <li><a href="../hacking.html">Ciberseguridad</a></li>
                        <li><a href="../devops.html">DevOps</a></li>
                    </ul>
                </nav>
            </div>
            <i class="fa-solid fa-bars" style="color: #ffffff;" id="icon_menu"></i>
            <div class="header__botonMenu">
                <input type="button" class="btn__header-botonMenu" value="Aportar" onclick="window.open('https://buymeacoffee.com/ryuzak1', '_blank');">
            </div>
        </div>
    </header>
    <main>
        <div class="cover">
            <div class="text__articulo-cover">
                <br>
                <br>
                <h1>Apache Airflow</h1>
                <p>Apache Airflow es una plataforma de c√≥digo abierto creada por Airbnb y gestionada por la Apache Software Foundation, que permite dise√±ar, programar y monitorizar flujos de trabajo (workflows) programables. Airflow no ejecuta c√≥digo por s√≠ mismo, sino que coordina cu√°ndo, c√≥mo y en qu√© orden deben ejecutarse tareas definidas por el usuario. Es una herramienta fundamental en entornos de ingenier√≠a de datos moderna, donde se requiere que procesos complejos se ejecuten en etapas secuenciales o paralelas con condiciones l√≥gicas.</p>
                <p>Airflow es √∫til porque permite automatizar y gestionar pipelines de datos complejos con l√≥gica condicional, dependencias entre tareas, manejo de errores, reintentos, programaci√≥n peri√≥dica y visibilidad total desde una interfaz web. Su dise√±o modular y extensible permite integrarse con servicios en la nube, bases de datos, APIs, sistemas de ficheros, y m√°s. Adem√°s, est√° pensado para escalar horizontalmente y adaptarse tanto a peque√±os scripts diarios como a grandes flujos de datos empresariales.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow1.png" alt="">
                </div>
                <h2>Arquitectura</h2>
                <p>Airflow se compone de varios servicios que trabajan juntos:</p>
                <ul style="list-style-type: none;">
                    <li>ü™Å <marcador class="resaltado8">Scheduler: </marcador>Escanea los DAGs y programa las tareas seg√∫n su definici√≥n temporal y dependencias.</li>
                    <li>ü™Å <marcador class="resaltado8">Webserver: </marcador>Proporciona la interfaz web para visualizar el estado de los DAGs, logs, ejecuciones pasadas y detalles de las tareas.</li>
                    <li>ü™Å <marcador class="resaltado8">Worker(s): </marcador>Ejecutan las tareas que han sido programadas. Pueden escalarse horizontalmente.</li>
                    <li>ü™Å <marcador class="resaltado8">Metadata Database: </marcador>Guarda toda la informaci√≥n sobre DAGs, tareas, logs, estados y configuraciones. Usa PostgreSQL o MySQL.</li>
                    <li>ü™Å <marcador class="resaltado8">Triggerer (a partir de Airflow 2): </marcador>Gestiona los Deferrable Operators y reduce el consumo de recursos al esperar eventos externos.</li>
                </ul>
                <div class="blog-image-zoom">
                    <img src="media/airflow2.gif" alt="">
                </div>
                <h2>Componentes Fundamentales</h2>
                <h6>DAGs (Directed Acyclic Graphs)</h6>
                <p>Los DAGs son la unidad principal de Airflow. Representan un flujo de trabajo como un grafo dirigido sin ciclos. Cada nodo es una tarea, y las aristas representan dependencias entre ellas. Est√°n definidos en archivos Python que se cargan din√°micamente.</p>
                <div class="archivo">
from airflow import DAG
from datetime import datetime

dag = DAG(
    dag_id="mi_pipeline",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily"
)
                </div>
                <h6>Tasks y Operators</h6>
                <p>Una Task es una unidad de trabajo. Se define a trav√©s de un Operator, que es una plantilla que encapsula una acci√≥n espec√≠fica.</p>
                <ul style="list-style-type: none;">
                    <li>üìë <marcador class="resaltado8">PythonOperator: </marcador>ejecuta una funci√≥n Python.</li>
                    <li>üìë <marcador class="resaltado8">BashOperator: </marcador>ejecuta comandos de bash.</li>
                    <li>üìë <marcador class="resaltado8">EmailOperator: </marcador>env√≠a correos.</li>
                    <li>üìë <marcador class="resaltado8">DockerOperator: </marcador>ejecuta tareas dentro de un contenedor Docker.</li>
                    <li>üìë <marcador class="resaltado8">KubernetesPodOperator: </marcador>ejecuta pods en Kubernetes.</li>
                </ul>
                <div class="blog-image-zoom">
                    <img src="media/airflow3.png" alt="">
                </div>
                <div class="blog-image-zoom">
                    <img src="media/airflow5.gif" alt="">
                </div>
                <h6>Sensors</h6>
                <p>Los Sensors son tareas que esperan a que ocurra una condici√≥n externa. Por ejemplo, un archivo aparezca, una tabla est√© disponible, o una API responda. Existen sensores como FileSensor, S3KeySensor, ExternalTaskSensor.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow4.png" alt="">
                </div>
                <h6>Deferrable Operators y Triggerer</h6>
                <p>A partir de Airflow 2, se introdujeron los Deferrable Operators, que permiten suspender una tarea en espera de un evento sin ocupar un worker. Estas tareas son gestionadas por el Triggerer, que utiliza async IO para mantenerlas vivas de manera eficiente.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow6.png" alt="">
                </div>
                <h6>XCom (Cross-Communication)</h6>
                <p>XCom (Cross-Communication) es el mecanismo de Airflow para compartir peque√±os datos entre tareas de un mismo DAG. Los datos se almacenan en la base de datos de Airflow (tabla xcom).</p>
                <p>üìÑ Puedes ‚Äúempujar‚Äù un valor desde una tarea y ‚Äújalarlo‚Äù desde otra.</p>
                <div class="archivo">
context['ti'].xcom_push(key='clave', value='valor', execution_date=otra_fecha)
valor = context['ti'].xcom_pull(key='clave', task_ids='tarea_origen')                </div>
                <ul style="list-style-type: none;">
                    <li>üî∏<marcador class="resaltado8">xcom_push()</marcador></li>
                    <p>üìÑ Se usa para enviar datos. Internamente, Airflow ejecuta:</p>
                    <div class="archivo">
XCom.set(
    key="mi_clave", 
    value="mi_valor", 
    task_id=task_instance.task_id,
    dag_id=task_instance.dag_id,
    execution_date=task_instance.execution_date  # ¬°Importante!
)
                    </div>
                    <ul style="list-style-type: none;">
                        <li>‚àò <marcador class="resaltado8">execution_date: </marcador>Siempre se guarda en XCom y determina a qu√© ejecuci√≥n (DAGRun) pertenece el dato.</li>
                        <li>‚àò Puedes sobrescribir el execution_date.</li>
                    </ul>
                    <li>üî∏<marcador class="resaltado8">xcom_pull()</marcador></li>
                    <p>Se usa para recuperar datos. Por defecto, busca registros con:</p>
                    <ul style="list-style-type: none;">
                        <li>‚ä° Mismo dag_id (DAG actual).</li>
                        <li>‚ä° Mismo execution_date (solo datos de la ejecuci√≥n actual).</li>
                    </ul>
                    <p>üìÑ Por defecto, se filtran los resultados por la ejecuci√≥n actual.</p>
                    <div class="archivo">
SELECT * FROM xcom 
WHERE dag_id = ? AND execution_date = ? AND key = ?;
                    </div>
                    <p>‚àò No mezcla datos entre diferentes ejecuciones del DAG.</p>
                    <p>El execution_date es crucial porque Airflow, por defecto, a√≠sla los XComs entre ejecuciones, requiriendo que se especifique manualmente un execution_date diferente para acceder a datos de otros DAGRuns.</p>
                </ul>
                <p>No abuses de XCom, no est√° dise√±ado para datos grandes (usa sistemas externos como S3 o Redis para eso).</p>
                <div class="blog-image-grande">
                    <img src="media/airflow7.png" alt="">
                </div>
                <h6>Hooks y Providers</h6>
                <ul style="list-style-type: none;">
                    <li>üî∏<marcador class="resaltado8">Hooks: </marcador>Son interfaces reutilizables para interactuar con sistemas externos: S3, MySQL, BigQuery, etc.</li>
                    <p>Los hooks, tambi√©n conocidos como "conectores" o "enlaces", son componentes fundamentales en Airflow que act√∫an como interfaces, puentes o conectores entre Airflow y sistemas externos. Estos hooks permiten la interacci√≥n, conexi√≥n y comunicaci√≥n con diversas plataformas, bases de datos y servicios.</p>
                    <p>Caracter√≠sticas clave:</p>
                    <ul style="list-style-type: none;">
                        <li>‚àò <marcador class="resaltado8">Interfaz unificada: </marcador>Proporcionan una forma estandarizada de interactuar con sistemas externos.</li>
                        <li>‚àò <marcador class="resaltado8">Manejo de conexiones: </marcador>Gestionan autom√°ticamente las conexiones, sesiones y autenticaciones.</li>
                        <li>‚àò <marcador class="resaltado8">Reutilizables: </marcador>Pueden ser usados m√∫ltiples veces en diferentes tareas y DAGs.</li>
                    </ul>
                    <p>Tipos Comunes de Hooks:</p>
                    <ol>
                        <li><marcador class="resaltado9">Database Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">PostgresHook: </marcador>Para PostgreSQL (conexiones a PostgreSQL).</li>
                            <li>‚àò <marcador class="resaltado8">MySqlHook: </marcador>Para MySQL (gesti√≥n de MySQL).</li>
                        </ul>
                        <li><marcador class="resaltado9">Cloud Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">S3Hook: </marcador>Interact√∫a con Amazon S3 (almacenamiento en S3).</li>
                            <li>‚àò <marcador class="resaltado8">GCSHook: </marcador>Para Google Cloud Storage (acceso a GCS).</li>
                        </ul>
                        <li><marcador class="resaltado9">API Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">HttpHook: </marcador>Para llamadas HTTP (solicitudes HTTP).</li>
                            <li>‚àò <marcador class="resaltado8">SlackHook: </marcador>Notificaciones en Slack (mensajes a Slack).</li>
                        </ul>
                        <br>
                        <p>Los hooks de Airflow simplifican la interacci√≥n con sistemas externos al abstraer la l√≥gica de conexi√≥n, recuperando credenciales de las Connections de Airflow, estableciendo la conexi√≥n, ejecutando operaciones y cerr√°ndola autom√°ticamente al finalizar su uso.</p>
                    </ol>
                    <p>üìÑ Ejemplo de Uso:</p>
                    <div class="archivo">
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Crear instancia del hook (inicializaci√≥n del hook)
hook = PostgresHook(postgres_conn_id='mi_postgres')

# Ejecutar consulta (uso del hook para consultar)
resultados = hook.get_records("SELECT * FROM tabla")

# El hook maneja la conexi√≥n autom√°ticamente (sin necesidad de abrir/cerrar manualmente)     
                    </div>
                    <li>üî∏<marcador class="resaltado8">Providers: </marcador>Son paquetes que agrupan hooks, operators, sensors y configuraciones para integrar servicios como Google Cloud, AWS, Slack, Snowflake, etc.</li>
                </ul>

                <div class="blog-image-zoom">
                    <img src="media/airflow8.png" alt="">
                </div>
                <div class="blog-image-zoom">
                    <img src="media/airflow9.png" alt="">
                </div>
                <h6>Variables y Connections</h6>
                <ul style="list-style-type: none;">
                    <li>‚àò <marcador class="resaltado8">Variables: </marcador>Claves/valores que se almacenan en la base de datos y pueden usarse desde cualquier DAG.</li>
                    <li>‚àò <marcador class="resaltado8">Connections: </marcador>Configuraciones predefinidas de conexi√≥n con servicios externos (host, puerto, usuario, contrase√±a, etc.).</li>
                </ul>
                <p>üìÑ Estas se configuran desde la UI o v√≠a CLI:</p>
                <div class="archivo">
airflow variables set nombre valor
airflow connections add my_db --conn-uri postgres://user:pass@host/db    
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow10.png" alt="">
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow11.png" alt="">
                </div>
                <h6>Trigger Rules y Condicionales</h6>
                <p>Las Trigger Rules controlan cu√°ndo se ejecuta una tarea seg√∫n el estado de las tareas anteriores. Ejemplo: all_success, one_failed, all_done, etc.</p>
                <p>üìÑ Puedes crear flujos condicionales din√°micos usando operadores como BranchPythonOperator:</p>
                <div class="archivo">
from airflow.operators.python import BranchPythonOperator

def elegir_ruta():
    return "task_a" if condicion else "task_b"

BranchPythonOperator(
    task_id='elige',
    python_callable=elegir_ruta,
    dag=dag
)
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow12.jpeg" alt="">
                </div>
                <h6>Setup y Teardown</h6>
                <p>A partir de Airflow 2.6, se introdujo setup y teardown para definir tareas que deben ejecutarse al inicio o al final de un DAG (como preparar o limpiar recursos), independientemente del √©xito o fallo de otras tareas.</p>
                <div class="archivo">
@dag.setup()
def inicializar():
    ...

@dag.teardown()
def limpiar():
    ...
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow13.png" alt="">
                </div>
                <h6>Decorators</h6>
                <p>Los Decorators (@task, @dag) permiten definir tareas y DAGs de forma m√°s limpia y funcional desde Python puro.</p>
                <div class="archivo">
from airflow.decorators import dag, task

@task
def suma(a, b):
    return a + b

@dag(schedule="@daily", start_date=datetime(2024, 1, 1))
def flujo():
    suma(3, 5)

flujo_dag = flujo()
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow14.jpeg" alt="">
                </div>
                <h2>Proyecto B√°sico en Linux</h2>
                <div class="contenedor">
                    <div class="etiqueta">
                        Creaci√≥n de un entorno virtual:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="python3 -m venv venv" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Activar el entorno virtual:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="source venv/bin/activate" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Puedes consultar la documentaci√≥n oficial de <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html" target="blank">Apache Airflow</a> para instalarlo utilizando Docker Compose.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Descarga del archivo docker-compose.yaml:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.0.1/docker-compose.yaml'" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Creaci√≥n de carpetas del entorno:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="mkdir -p ./dags ./ logs ./plugins ./config" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>En tu m√°quina, puedes corregir los permisos (50000:1000) para que el contenedor corra como un usuario no-root.</p>
                <p>Edita el archivo <marcador class="resaltado1">/etc/containers/registries.conf</marcador>:</p>
                <div class="archivo">
[registries.search]
registries = ['docker.io']
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Asignaci√≥n de variable de entorno:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="echo &quot;AIRFLOW_UID=$(id -u)&quot; | sponge .env" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Revisa con <marcador class="subrayado">cat .env</marcador> que el UID tenga un valor num√©rico; si no, establ√©celo manualmente.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Levantar el servicio:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="podman-compose up airflow-init" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Levantar la interfaz web:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="podman-compose up -d" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">Interfaz web de Airflow:</div>
                    <div class="comandos">
                        <input type="text" class="text" value="http://localhost:8080/" oninput="ajustarancho(this)">
                        <button><i class="fa fa-clone"></i><span> copy</span></button>
                    </div>
                </div>
                <p>La contrase√±a predeterminada para el usuario airflow en las instalaciones iniciales de Apache Airflow es tambi√©n airflow.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow15.png" alt="">
                </div>
                <p>‚å≠ Modificar el <marcador class="resaltado1">docker-compose.yaml</marcador> para a√±adir una base de datos.</p>
                <div class="archivo">            
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    ports:
      - "5432:5432"

  pgadmin:
    container_name: pgadmin4_container2
    image: dpage/pgadmin4
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - "5050:80"
                </div>
            <div class="contenedor">
                <div class="etiqueta">
                    Detener el servicio:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="podman-compose down" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">
                    Levantar Airflow:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="podman-compose up -d" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Interfaz web de Airflow:</div>
                <div class="comandos">
                    <input type="text" class="text" value="http://localhost:5050/" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>La contrase√±a predeterminada para el usuario admin@admin.com en la base de datos pgAdmin es root.</p>
            <div class="blog-image-grande">
                <img src="media/airflow16.png" alt="">
            </div>
            <p>‚ùè Buscar el contenedor postgre.</p>
            <div id="terminal">
                <section id="terminal__bar">
                    <div id="bar__buttons">
                    <button class="bar__button" id="bar__button--exit">&#10005;</button>
                    <button class="bar__button">&#9633;</button>
                    <button class="bar__button">&#9472;</button>
                    </div>
                    <p id="bar__user">ryuzak1@ubuntu: ~</p>
                </section>
                <section id="terminal__body">
                    <div id="terminal__prompt">
                    <span id="terminal__prompt--command"><marcador class="user">ryuzak1@ubuntu:</marcador><marcador class="location">~</marcador><marcador class="bling">$&nbsp;</marcador><marcador class="tool">podman</marcador> ps <marcador class="param">-a</marcador></span>
                    

                    </div>
                    <div class="command-response-container">
                        <span id="terminal__prompt--response" class="multiline-text">CONTAINER ID  IMAGE                                 COMMAND               CREATED        STATUS                    PORTS                   NAMES
3398a6229828  docker.io/library/postgres:13         postgres              9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:5432->5432/tcp  airflow_postgres_1
9c6544ddbcdd  docker.io/dpage/pgadmin4:latest                             9 minutes ago  Up 9 minutes              0.0.0.0:5050->80/tcp    pgadmin4_container2
514587874e67  docker.io/library/redis:7.2-bookworm  redis-server          9 minutes ago  Up 9 minutes (healthy)                            airflow_redis_1
a5aa5aa7161b  docker.io/apache/airflow:3.0.1        -c if [[ -z "1000...  9 minutes ago  Exited (0) 8 minutes ago                          airflow_airflow-init_1
f2d305d2d01d  docker.io/apache/airflow:3.0.1        bash -c airflow       9 minutes ago  Exited (2) 9 minutes ago                          airflow_airflow-cli_1
d32d734888a4  docker.io/apache/airflow:3.0.1        api-server            9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:8080->8080/tcp  airflow_airflow-apiserver_1
6544fcc60245  docker.io/apache/airflow:3.0.1        scheduler             9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-scheduler_1
e94074a424a0  docker.io/apache/airflow:3.0.1        dag-processor         9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-dag-processor_1
8ec81ef33dc1  docker.io/apache/airflow:3.0.1        triggerer             9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-triggerer_1
256a2bb757be  docker.io/apache/airflow:3.0.1        celery flower         9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:5555->5555/tcp  airflow_flower_1
445a415b36a9  docker.io/apache/airflow:3.0.1        celery worker         9 minutes ago  Up 9 minutes (unhealthy)                          airflow_airflow-worker_1                        
                        </span>
                    </div>
                </section>
            </div>
            <p>‚ùè Buscar la IP del contenedor postgre.</p>
            <div id="terminal">
                <section id="terminal__bar">
                    <div id="bar__buttons">
                    <button class="bar__button" id="bar__button--exit">&#10005;</button>
                    <button class="bar__button">&#9633;</button>
                    <button class="bar__button">&#9472;</button>
                    </div>
                    <p id="bar__user">ryuzak1@ubuntu: ~</p>
                </section>
                <section id="terminal__body">
                    <div id="terminal__prompt">
                    <span id="terminal__prompt--command"><marcador class="user">ryuzak1@ubuntu:</marcador><marcador class="location">~</marcador><marcador class="bling">$&nbsp;</marcador><marcador class="tool">podman</marcador> inspect 3398a6229828 | <marcador class="tool">jq</marcador> '.[0].NetworkSettings.Networks'<marcador class="param"></marcador></span>

                    </div>
                    <div class="command-response-container">
                        <span id="terminal__prompt--response" class="multiline-text">{
    "airflow_default": {
        "EndpointID": "",
        "Gateway": "10.89.0.1",
        "IPAddress": "10.89.0.92",
        "IPPrefixLen": 24,
        "IPv6Gateway": "",
        "GlobalIPv6Address": "",
        "GlobalIPv6PrefixLen": 0,
        "MacAddress": "92:42:1f:c3:21:99",
        "NetworkID": "airflow_default",
        "DriverOpts": null,
        "IPAMConfig": null,
        "Links": null,
        "Aliases": [
        "postgres",
        "3398a6229828"
        ]
    }
}
                        </span>
                    </div>
                </section>
            </div>
            <p>A√±adir un nuevo servidor con los siguientes valores: en Name, poner 'ps_db'. En la secci√≥n de Connections, colocar la IP en el campo Hostname, y 'airflow' en Username y Password. Por √∫ltimo, hacer clic en Save.</p>
            <div class="blog-image-zoom">
                <img src="media/airflow17.png" alt="">
            </div>
            <p>Crea una base de datos haciendo clic derecho sobre "Databases" y simplemente establece el nombre 'amazon_books'.</p>
            <p>En la secci√≥n de "Admin" y luego en "Connections", crea una nueva conexi√≥n con los siguientes valores:</p>
            <div class="blog-image-zoom">
                <img src="media/airflow18.png" alt="">
            </div>
            <p>‚å≠ Crea un script llamado dag.py en la carpeta dags y modifica la fecha.</p>
            <div class="archivo">
from datetime import datetime, timedelta
from airflow import DAG
import requests
import pandas as pd
from bs4 import BeautifulSoup
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Headers que funcionaron en el script exitoso
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9"
}

def get_amazon_data_books(num_books, ti):
    print(f"üöÄ Iniciando scraping para {num_books} libros")
    base_url = "https://www.amazon.com/s?k=data+engineering+books"
    books = []
    page = 1
    
    while len(books) < num_books:
        url = f"{base_url}&page={page}"
        print(f"üìñ Procesando p√°gina {page}: {url}")
        
        response = requests.get(url, headers=headers, timeout=30)
        print(f"üîç Status code: {response.status_code}")
        
        if response.status_code != 200:
            print(f"‚ùå Error en la petici√≥n: {response.status_code}")
            break
            
        soup = BeautifulSoup(response.content, "html.parser")
        book_containers = soup.find_all('div', {'data-component-type': 's-search-result'})
        print(f"üìö Encontrados {len(book_containers)} contenedores de libros")
        
        for book in book_containers:
            try:
                title = book.find('h2').text.strip() if book.find('h2') else None
                author = book.find('a', {'class': 'a-size-base'})
                price = book.find('span', {'class': 'a-price-whole'})
                rating = book.find('span', {'class': 'a-icon-alt'})
                
                if title and author and price and rating:
                    books.append({
                        "Title": title[:100],  # Limitar longitud para DB
                        "Author": author.text.strip(),
                        "Price": price.text.strip(),
                        "Rating": rating.text.split()[0]  # Solo el n√∫mero
                    })
                    print(f"‚úÖ A√±adido: {title[:30]}...")
            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando libro: {str(e)}")
                continue
                
        page += 1
        if page > 3:  # L√≠mite de p√°ginas para pruebas
            break
    
    df = pd.DataFrame(books[:num_books])
    print(f"üìä Total de libros obtenidos: {len(df)}")
    ti.xcom_push(key='book_data', value=df.to_dict('records'))

def insert_book_data_into_postgres(ti):
    book_data = ti.xcom_pull(key='book_data', task_ids='fetch_book_data')
    print(f"üì• Datos recibidos para insertar: {len(book_data) if book_data else 0} registros")
    
    if not book_data:
        raise ValueError("No se encontraron datos de libros")
    
    hook = PostgresHook(postgres_conn_id='books_connection')
    conn = hook.get_conn()
    cursor = conn.cursor()
    
    try:
        for book in book_data:
            cursor.execute("""
                INSERT INTO books (title, authors, price, rating)
                VALUES (%s, %s, %s, %s)
            """, (book['Title'], book['Author'], book['Price'], book['Rating']))
        conn.commit()
        print(f"üíæ Insertados {len(book_data)} registros en PostgreSQL")
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error en la inserci√≥n: {str(e)}")
        raise
    finally:
        cursor.close()
        conn.close()

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 5, 14),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='fetch_and_store_amazon_books',
    default_args=default_args,
    description='A simple DAG to fetch book data from Amazon and store it in Postgres',
    schedule=timedelta(days=1),
)

#operators : Python Operator and PostgresOperator
#hooks - allows connection to postgres


fetch_book_data_task = PythonOperator(
    task_id='fetch_book_data',
    python_callable=get_amazon_data_books,
    op_args=[50],  # Number of books to fetch
    dag=dag,
)

create_table_task = SQLExecuteQueryOperator(
    task_id='create_table',
    conn_id='books_connection',
    sql="""
    CREATE TABLE IF NOT EXISTS books (
        id SERIAL PRIMARY KEY,
        title TEXT NOT NULL,
        authors TEXT,
        price TEXT,
        rating TEXT
    );
    """,
    dag=dag,
)

insert_book_data_task = PythonOperator(
    task_id='insert_book_data',
    python_callable=insert_book_data_into_postgres,
    dag=dag,
)

#dependencies

fetch_book_data_task >> create_table_task >> insert_book_data_task
            </div>
            <p>En la secci√≥n de DAGs y buscar por el DAG llamado fetch_and_store_amazon_book; una vez dentro, dar clic en el bot√≥n de "Play" para ejecutar la tarea.</p>
            <p>Ve a pgAdmin para verificar los datos obtenidos de Amazon. Haz clic derecho en "Tables" y selecciona "Query Tool".</p>
            <div class="blog-image-zoom">
                <img src="media/airflow19.png" alt="">
            </div>
            </div>
        </div>       
    </main>
    <script src="../js/script.js"></script>
</body>
</html>