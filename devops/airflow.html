<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog de Tecnolog√≠a</title>
    <link rel="icon" href="../media/imagen41.png" type="image/png">
    <script src="https://kit.fontawesome.com/9474e300a6.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="../css/estilos-articulo.css">
</head>
<body>

    <header>
        <div class="container__header">
            <div class="logo">
                <img src="../media/logo-1.png" alt="">
            </div>

            <div class="menu">
                <nav>
                    <ul>
                        <li><a href="../index.html">Inicio</a></li>
                        <li><a href="../electronica.html">Electr√≥nica</a></li>
                        <li><a href="../network.html">Networking</a></li>
                        <li><a href="../ia.html">Inteligencia Artificial</a></li>
                        <li><a href="../hacking.html">Ciberseguridad</a></li>
                        <li><a href="../devops.html">DevOps</a></li>
                    </ul>
                </nav>
            </div>
            <i class="fa-solid fa-bars" style="color: #ffffff;" id="icon_menu"></i>
            <div class="header__botonMenu">
                <input type="button" class="btn__header-botonMenu" value="Aportar" onclick="window.open('https://buymeacoffee.com/ryuzak1', '_blank');">
            </div>
        </div>
    </header>
    <main>
        <div class="cover">
            <div class="text__articulo-cover">
                <br>
                <br>
                <h1>Apache Airflow</h1>
                <p>Apache Airflow es una plataforma de c√≥digo abierto creada por Airbnb y gestionada por la Apache Software Foundation, que permite dise√±ar, programar y monitorizar flujos de trabajo (workflows) programables. Airflow no ejecuta c√≥digo por s√≠ mismo, sino que coordina cu√°ndo, c√≥mo y en qu√© orden deben ejecutarse tareas definidas por el usuario. Es una herramienta fundamental en entornos de ingenier√≠a de datos moderna, donde se requiere que procesos complejos se ejecuten en etapas secuenciales o paralelas con condiciones l√≥gicas.</p>
                <p>Airflow es √∫til porque permite automatizar y gestionar pipelines de datos complejos con l√≥gica condicional, dependencias entre tareas, manejo de errores, reintentos, programaci√≥n peri√≥dica y visibilidad total desde una interfaz web. Su dise√±o modular y extensible permite integrarse con servicios en la nube, bases de datos, APIs, sistemas de ficheros, y m√°s. Adem√°s, est√° pensado para escalar horizontalmente y adaptarse tanto a peque√±os scripts diarios como a grandes flujos de datos empresariales.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow1.png" alt="">
                </div>
                <h2>Arquitectura</h2>
                <p>Airflow se compone de varios servicios que trabajan juntos:</p>
                <ul style="list-style-type: none;">
                    <li>ü™Å <marcador class="resaltado8">Scheduler: </marcador>Escanea los DAGs y programa las tareas seg√∫n su definici√≥n temporal y dependencias.</li>
                    <li>ü™Å <marcador class="resaltado8">Webserver: </marcador>Proporciona la interfaz web para visualizar el estado de los DAGs, logs, ejecuciones pasadas y detalles de las tareas.</li>
                    <li>ü™Å <marcador class="resaltado8">Worker(s): </marcador>Ejecutan las tareas que han sido programadas. Pueden escalarse horizontalmente.</li>
                    <li>ü™Å <marcador class="resaltado8">Metadata Database: </marcador>Guarda toda la informaci√≥n sobre DAGs, tareas, logs, estados y configuraciones. Usa PostgreSQL o MySQL.</li>
                    <li>ü™Å <marcador class="resaltado8">Triggerer (a partir de Airflow 2): </marcador>Gestiona los Deferrable Operators y reduce el consumo de recursos al esperar eventos externos.</li>
                </ul>
                <div class="blog-image-zoom">
                    <img src="media/airflow2.gif" alt="">
                </div>
                <h2>Componentes Fundamentales</h2>
                <h6>DAGs (Directed Acyclic Graphs)</h6>
                <p>Los DAGs son la unidad principal de Airflow. Representan un flujo de trabajo como un grafo dirigido sin ciclos. Cada nodo es una tarea, y las aristas representan dependencias entre ellas. Est√°n definidos en archivos Python que se cargan din√°micamente.</p>
                <div class="archivo">
from airflow import DAG
from datetime import datetime

dag = DAG(
    dag_id="mi_pipeline",
    start_date=datetime(2024, 1, 1),
    schedule_interval="@daily"
)
                </div>
                <h6>Tasks y Operators</h6>
                <p>Una Task es una unidad de trabajo. Se define a trav√©s de un Operator, que es una plantilla que encapsula una acci√≥n espec√≠fica.</p>
                <ul style="list-style-type: none;">
                    <li>üìë <marcador class="resaltado8">PythonOperator: </marcador>ejecuta una funci√≥n Python.</li>
                    <li>üìë <marcador class="resaltado8">BashOperator: </marcador>ejecuta comandos de bash.</li>
                    <li>üìë <marcador class="resaltado8">EmailOperator: </marcador>env√≠a correos.</li>
                    <li>üìë <marcador class="resaltado8">DockerOperator: </marcador>ejecuta tareas dentro de un contenedor Docker.</li>
                    <li>üìë <marcador class="resaltado8">KubernetesPodOperator: </marcador>ejecuta pods en Kubernetes.</li>
                </ul>
                <div class="blog-image-zoom">
                    <img src="media/airflow3.png" alt="">
                </div>
                <div class="blog-image-zoom">
                    <img src="media/airflow5.gif" alt="">
                </div>
                <h6>Sensors</h6>
                <p>Los Sensors son tareas que esperan a que ocurra una condici√≥n externa. Por ejemplo, un archivo aparezca, una tabla est√© disponible, o una API responda. Existen sensores como FileSensor, S3KeySensor, ExternalTaskSensor.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow4.png" alt="">
                </div>
                <h6>Deferrable Operators y Triggerer</h6>
                <p>A partir de Airflow 2, se introdujeron los Deferrable Operators, que permiten suspender una tarea en espera de un evento sin ocupar un worker. Estas tareas son gestionadas por el Triggerer, que utiliza async IO para mantenerlas vivas de manera eficiente.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow6.png" alt="">
                </div>
                <h6>XCom (Cross-Communication)</h6>
                <p>XCom (Cross-Communication) es el mecanismo de Airflow para compartir peque√±os datos entre tareas de un mismo DAG. Los datos se almacenan en la base de datos de Airflow (tabla xcom).</p>
                <p>üìÑ Puedes ‚Äúempujar‚Äù un valor desde una tarea y ‚Äújalarlo‚Äù desde otra.</p>
                <div class="archivo">
context['ti'].xcom_push(key='clave', value='valor', execution_date=otra_fecha)
valor = context['ti'].xcom_pull(key='clave', task_ids='tarea_origen')                </div>
                <ul style="list-style-type: none;">
                    <li>üî∏<marcador class="resaltado8">xcom_push()</marcador></li>
                    <p>üìÑ Se usa para enviar datos. Internamente, Airflow ejecuta:</p>
                    <div class="archivo">
XCom.set(
    key="mi_clave", 
    value="mi_valor", 
    task_id=task_instance.task_id,
    dag_id=task_instance.dag_id,
    execution_date=task_instance.execution_date  # ¬°Importante!
)
                    </div>
                    <ul style="list-style-type: none;">
                        <li>‚àò <marcador class="resaltado8">execution_date: </marcador>Siempre se guarda en XCom y determina a qu√© ejecuci√≥n (DAGRun) pertenece el dato.</li>
                        <li>‚àò Puedes sobrescribir el execution_date.</li>
                    </ul>
                    <li>üî∏<marcador class="resaltado8">xcom_pull()</marcador></li>
                    <p>Se usa para recuperar datos. Por defecto, busca registros con:</p>
                    <ul style="list-style-type: none;">
                        <li>‚ä° Mismo dag_id (DAG actual).</li>
                        <li>‚ä° Mismo execution_date (solo datos de la ejecuci√≥n actual).</li>
                    </ul>
                    <p>üìÑ Por defecto, se filtran los resultados por la ejecuci√≥n actual.</p>
                    <div class="archivo">
SELECT * FROM xcom 
WHERE dag_id = ? AND execution_date = ? AND key = ?;
                    </div>
                    <p>‚àò No mezcla datos entre diferentes ejecuciones del DAG.</p>
                    <p>El execution_date es crucial porque Airflow, por defecto, a√≠sla los XComs entre ejecuciones, requiriendo que se especifique manualmente un execution_date diferente para acceder a datos de otros DAGRuns.</p>
                </ul>
                <p>No abuses de XCom, no est√° dise√±ado para datos grandes (usa sistemas externos como S3 o Redis para eso).</p>
                <div class="blog-image-grande">
                    <img src="media/airflow7.png" alt="">
                </div>
                <h6>Hooks y Providers</h6>
                <ul style="list-style-type: none;">
                    <li>üî∏<marcador class="resaltado8">Hooks: </marcador>Son interfaces reutilizables para interactuar con sistemas externos: S3, MySQL, BigQuery, etc.</li>
                    <p>Los hooks, tambi√©n conocidos como "conectores" o "enlaces", son componentes fundamentales en Airflow que act√∫an como interfaces, puentes o conectores entre Airflow y sistemas externos. Estos hooks permiten la interacci√≥n, conexi√≥n y comunicaci√≥n con diversas plataformas, bases de datos y servicios.</p>
                    <p>Caracter√≠sticas clave:</p>
                    <ul style="list-style-type: none;">
                        <li>‚àò <marcador class="resaltado8">Interfaz unificada: </marcador>Proporcionan una forma estandarizada de interactuar con sistemas externos.</li>
                        <li>‚àò <marcador class="resaltado8">Manejo de conexiones: </marcador>Gestionan autom√°ticamente las conexiones, sesiones y autenticaciones.</li>
                        <li>‚àò <marcador class="resaltado8">Reutilizables: </marcador>Pueden ser usados m√∫ltiples veces en diferentes tareas y DAGs.</li>
                    </ul>
                    <p>Tipos Comunes de Hooks:</p>
                    <ol>
                        <li><marcador class="resaltado9">Database Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">PostgresHook: </marcador>Para PostgreSQL (conexiones a PostgreSQL).</li>
                            <li>‚àò <marcador class="resaltado8">MySqlHook: </marcador>Para MySQL (gesti√≥n de MySQL).</li>
                        </ul>
                        <li><marcador class="resaltado9">Cloud Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">S3Hook: </marcador>Interact√∫a con Amazon S3 (almacenamiento en S3).</li>
                            <li>‚àò <marcador class="resaltado8">GCSHook: </marcador>Para Google Cloud Storage (acceso a GCS).</li>
                        </ul>
                        <li><marcador class="resaltado9">API Hooks:</marcador></li>
                        <ul style="list-style-type: none;">
                            <li>‚àò <marcador class="resaltado8">HttpHook: </marcador>Para llamadas HTTP (solicitudes HTTP).</li>
                            <li>‚àò <marcador class="resaltado8">SlackHook: </marcador>Notificaciones en Slack (mensajes a Slack).</li>
                        </ul>
                        <br>
                        <p>Los hooks de Airflow simplifican la interacci√≥n con sistemas externos al abstraer la l√≥gica de conexi√≥n, recuperando credenciales de las Connections de Airflow, estableciendo la conexi√≥n, ejecutando operaciones y cerr√°ndola autom√°ticamente al finalizar su uso.</p>
                    </ol>
                    <p>üìÑ Ejemplo de Uso:</p>
                    <div class="archivo">
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Crear instancia del hook (inicializaci√≥n del hook)
hook = PostgresHook(postgres_conn_id='mi_postgres')

# Ejecutar consulta (uso del hook para consultar)
resultados = hook.get_records("SELECT * FROM tabla")

# El hook maneja la conexi√≥n autom√°ticamente (sin necesidad de abrir/cerrar manualmente)     
                    </div>
                    <li>üî∏<marcador class="resaltado8">Providers: </marcador>Son paquetes que agrupan hooks, operators, sensors y configuraciones para integrar servicios como Google Cloud, AWS, Slack, Snowflake, etc.</li>
                </ul>

                <div class="blog-image-zoom">
                    <img src="media/airflow8.png" alt="">
                </div>
                <div class="blog-image-zoom">
                    <img src="media/airflow9.png" alt="">
                </div>
                <h6>Variables y Connections</h6>
                <ul style="list-style-type: none;">
                    <li>‚àò <marcador class="resaltado8">Variables: </marcador>Claves/valores que se almacenan en la base de datos y pueden usarse desde cualquier DAG.</li>
                    <li>‚àò <marcador class="resaltado8">Connections: </marcador>Configuraciones predefinidas de conexi√≥n con servicios externos (host, puerto, usuario, contrase√±a, etc.).</li>
                </ul>
                <p>üìÑ Estas se configuran desde la UI o v√≠a CLI:</p>
                <div class="archivo">
airflow variables set nombre valor
airflow connections add my_db --conn-uri postgres://user:pass@host/db    
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow10.png" alt="">
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow11.png" alt="">
                </div>
                <h6>Trigger Rules y Condicionales</h6>
                <p>Las Trigger Rules controlan cu√°ndo se ejecuta una tarea seg√∫n el estado de las tareas anteriores. Ejemplo: all_success, one_failed, all_done, etc.</p>
                <p>üìÑ Puedes crear flujos condicionales din√°micos usando operadores como BranchPythonOperator:</p>
                <div class="archivo">
from airflow.operators.python import BranchPythonOperator

def elegir_ruta():
    return "task_a" if condicion else "task_b"

BranchPythonOperator(
    task_id='elige',
    python_callable=elegir_ruta,
    dag=dag
)
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow12.jpeg" alt="">
                </div>
                <h6>Setup y Teardown</h6>
                <p>A partir de Airflow 2.6, se introdujo setup y teardown para definir tareas que deben ejecutarse al inicio o al final de un DAG (como preparar o limpiar recursos), independientemente del √©xito o fallo de otras tareas.</p>
                <div class="archivo">
@dag.setup()
def inicializar():
    ...

@dag.teardown()
def limpiar():
    ...
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow13.png" alt="">
                </div>
                <h6>Decorators</h6>
                <p>Los Decorators (@task, @dag) permiten definir tareas y DAGs de forma m√°s limpia y funcional desde Python puro.</p>
                <div class="archivo">
from airflow.decorators import dag, task

@task
def suma(a, b):
    return a + b

@dag(schedule="@daily", start_date=datetime(2024, 1, 1))
def flujo():
    suma(3, 5)

flujo_dag = flujo()
                </div>
                <div class="blog-image-grande">
                    <img src="media/airflow14.jpeg" alt="">
                </div>
                <h2>Proyecto con Docker Compose en Linux</h2>
                <div class="contenedor">
                    <div class="etiqueta">
                        Creaci√≥n de un entorno virtual:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="python3 -m venv venv" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Activar el entorno virtual:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="source venv/bin/activate" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Puedes consultar la documentaci√≥n oficial de <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html" target="blank">Apache Airflow</a> para instalarlo utilizando Docker Compose.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Descarga del archivo docker-compose.yaml:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.0.1/docker-compose.yaml'" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Creaci√≥n de carpetas del entorno:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="mkdir -p ./dags ./ logs ./plugins ./config" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>En tu m√°quina, puedes corregir los permisos (50000:1000) para que el contenedor corra como un usuario no-root.</p>
                <p>Edita el archivo <marcador class="resaltado1">/etc/containers/registries.conf</marcador>:</p>
                <div class="archivo">
[registries.search]
registries = ['docker.io']
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Asignaci√≥n de variable de entorno:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="echo &quot;AIRFLOW_UID=$(id -u)&quot; | sponge .env" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <p>Revisa con <marcador class="subrayado">cat .env</marcador> que el UID tenga un valor num√©rico; si no, establ√©celo manualmente.</p>
                <div class="contenedor">
                    <div class="etiqueta">
                        Levantar el servicio:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="podman-compose up airflow-init" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">
                        Levantar la interfaz web:
                    </div>
                    <div class="comandos">
                        <input type="text" class="text" value="podman-compose up -d" oninput="ajustarAncho(this)">
                        <button><i class="fa fa-clone"></i><span> Copy</span></button>
                    </div>
                </div>
                <div class="contenedor">
                    <div class="etiqueta">Interfaz web de Airflow:</div>
                    <div class="comandos">
                        <input type="text" class="text" value="http://localhost:8080/" oninput="ajustarancho(this)">
                        <button><i class="fa fa-clone"></i><span> copy</span></button>
                    </div>
                </div>
                <p>La contrase√±a predeterminada para el usuario airflow en las instalaciones iniciales de Apache Airflow es tambi√©n airflow.</p>
                <div class="blog-image-grande">
                    <img src="media/airflow15.png" alt="">
                </div>
                <p>‚å≠ Modificar el <marcador class="resaltado1">docker-compose.yaml</marcador> para a√±adir una base de datos.</p>
                <div class="archivo">            
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    ports:
      - "5432:5432"

  pgadmin:
    container_name: pgadmin4_container2
    image: dpage/pgadmin4
    restart: always
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    ports:
      - "5050:80"
                </div>
            <div class="contenedor">
                <div class="etiqueta">
                    Detener el servicio:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="podman-compose down" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">
                    Levantar Airflow:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="podman-compose up -d" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Interfaz web de pgAdmin:</div>
                <div class="comandos">
                    <input type="text" class="text" value="http://localhost:5050/" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>La contrase√±a predeterminada para el usuario admin@admin.com en el panel de login de la base de datos pgAdmin es root.</p>
            <div class="blog-image-grande">
                <img src="media/airflow16.png" alt="">
            </div>
            <p>‚ùè Buscar el contenedor postgre.</p>
            <div id="terminal">
                <section id="terminal__bar">
                    <div id="bar__buttons">
                    <button class="bar__button" id="bar__button--exit">&#10005;</button>
                    <button class="bar__button">&#9633;</button>
                    <button class="bar__button">&#9472;</button>
                    </div>
                    <p id="bar__user">ryuzak1@ubuntu: ~</p>
                </section>
                <section id="terminal__body">
                    <div id="terminal__prompt">
                    <span id="terminal__prompt--command"><marcador class="user">ryuzak1@ubuntu:</marcador><marcador class="location">~</marcador><marcador class="bling">$&nbsp;</marcador><marcador class="tool">podman</marcador> ps <marcador class="param">-a</marcador></span>
                    

                    </div>
                    <div class="command-response-container">
                        <span id="terminal__prompt--response" class="multiline-text">CONTAINER ID  IMAGE                                 COMMAND               CREATED        STATUS                    PORTS                   NAMES
3398a6229828  docker.io/library/postgres:13         postgres              9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:5432->5432/tcp  airflow_postgres_1
9c6544ddbcdd  docker.io/dpage/pgadmin4:latest                             9 minutes ago  Up 9 minutes              0.0.0.0:5050->80/tcp    pgadmin4_container2
514587874e67  docker.io/library/redis:7.2-bookworm  redis-server          9 minutes ago  Up 9 minutes (healthy)                            airflow_redis_1
a5aa5aa7161b  docker.io/apache/airflow:3.0.1        -c if [[ -z "1000...  9 minutes ago  Exited (0) 8 minutes ago                          airflow_airflow-init_1
f2d305d2d01d  docker.io/apache/airflow:3.0.1        bash -c airflow       9 minutes ago  Exited (2) 9 minutes ago                          airflow_airflow-cli_1
d32d734888a4  docker.io/apache/airflow:3.0.1        api-server            9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:8080->8080/tcp  airflow_airflow-apiserver_1
6544fcc60245  docker.io/apache/airflow:3.0.1        scheduler             9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-scheduler_1
e94074a424a0  docker.io/apache/airflow:3.0.1        dag-processor         9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-dag-processor_1
8ec81ef33dc1  docker.io/apache/airflow:3.0.1        triggerer             9 minutes ago  Up 9 minutes (healthy)                            airflow_airflow-triggerer_1
256a2bb757be  docker.io/apache/airflow:3.0.1        celery flower         9 minutes ago  Up 9 minutes (healthy)    0.0.0.0:5555->5555/tcp  airflow_flower_1
445a415b36a9  docker.io/apache/airflow:3.0.1        celery worker         9 minutes ago  Up 9 minutes (unhealthy)                          airflow_airflow-worker_1                        
                        </span>
                    </div>
                </section>
            </div>
            <p>‚ùè Buscar la IP del contenedor postgre.</p>
            <div id="terminal">
                <section id="terminal__bar">
                    <div id="bar__buttons">
                    <button class="bar__button" id="bar__button--exit">&#10005;</button>
                    <button class="bar__button">&#9633;</button>
                    <button class="bar__button">&#9472;</button>
                    </div>
                    <p id="bar__user">ryuzak1@ubuntu: ~</p>
                </section>
                <section id="terminal__body">
                    <div id="terminal__prompt">
                    <span id="terminal__prompt--command"><marcador class="user">ryuzak1@ubuntu:</marcador><marcador class="location">~</marcador><marcador class="bling">$&nbsp;</marcador><marcador class="tool">podman</marcador> inspect 3398a6229828 | <marcador class="tool">jq</marcador> '.[0].NetworkSettings.Networks'<marcador class="param"></marcador></span>

                    </div>
                    <div class="command-response-container">
                        <span id="terminal__prompt--response" class="multiline-text">{
    "airflow_default": {
        "EndpointID": "",
        "Gateway": "10.89.0.1",
        "IPAddress": "10.89.0.92",
        "IPPrefixLen": 24,
        "IPv6Gateway": "",
        "GlobalIPv6Address": "",
        "GlobalIPv6PrefixLen": 0,
        "MacAddress": "92:42:1f:c3:21:99",
        "NetworkID": "airflow_default",
        "DriverOpts": null,
        "IPAMConfig": null,
        "Links": null,
        "Aliases": [
        "postgres",
        "3398a6229828"
        ]
    }
}
                        </span>
                    </div>
                </section>
            </div>
            <p>Se tiene que a√±adir un nuevo servidor con los siguientes valores: en Name, poner 'ps_db'. En la secci√≥n de Connections, colocar la IP en el campo Hostname, y 'airflow' en Username y Password. Por √∫ltimo, hacer clic en Save.</p>
            <div class="blog-image-zoom">
                <img src="media/airflow17.png" alt="">
            </div>
            <p>El proyecto requiere una base de datos para almacenar la informaci√≥n extra√≠da de productos de la web de Amazon. Para crearla, haz clic derecho sobre "Databases" y establece el nombre 'amazon_books'.</p>
            <p>En la secci√≥n de "Admin" y luego en "Connections", crea una nueva conexi√≥n con los siguientes valores:</p>
            <div class="blog-image-zoom">
                <img src="media/airflow18.png" alt="">
            </div>
            <p>Este proyecto de Airflow automatiza la extracci√≥n de informaci√≥n de libros de ingenier√≠a de datos desde la p√°gina de resultados de b√∫squeda de Amazon. Utiliza Python para realizar el scraping web, parseando el HTML con BeautifulSoup para extraer t√≠tulos, autores, precios y calificaciones. Los datos extra√≠dos se almacenan temporalmente utilizando XComs y luego se insertan en una base de datos PostgreSQL. El flujo de trabajo incluye la creaci√≥n de la tabla en PostgreSQL (si no existe), la obtenci√≥n de los datos de Amazon y la posterior inserci√≥n de estos datos en la tabla, todo orquestado por Airflow de forma diaria.</p>
            <p>‚å≠ Crear un script con el nombre dag.py en la carpeta dags y modificar la fecha.</p>
            <div class="archivo">
from datetime import datetime, timedelta
from airflow import DAG
import requests
import pandas as pd
from bs4 import BeautifulSoup
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Headers que funcionaron en el script exitoso
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9"
}

def get_amazon_data_books(num_books, ti):
    print(f"üöÄ Iniciando scraping para {num_books} libros")
    base_url = "https://www.amazon.com/s?k=data+engineering+books"
    books = []
    page = 1
    
    while len(books) < num_books:
        url = f"{base_url}&page={page}"
        print(f"üìñ Procesando p√°gina {page}: {url}")
        
        response = requests.get(url, headers=headers, timeout=30)
        print(f"üîç Status code: {response.status_code}")
        
        if response.status_code != 200:
            print(f"‚ùå Error en la petici√≥n: {response.status_code}")
            break
            
        soup = BeautifulSoup(response.content, "html.parser")
        book_containers = soup.find_all('div', {'data-component-type': 's-search-result'})
        print(f"üìö Encontrados {len(book_containers)} contenedores de libros")
        
        for book in book_containers:
            try:
                title = book.find('h2').text.strip() if book.find('h2') else None
                author = book.find('a', {'class': 'a-size-base'})
                price = book.find('span', {'class': 'a-price-whole'})
                rating = book.find('span', {'class': 'a-icon-alt'})
                
                if title and author and price and rating:
                    books.append({
                        "Title": title[:100],  # Limitar longitud para DB
                        "Author": author.text.strip(),
                        "Price": price.text.strip(),
                        "Rating": rating.text.split()[0]  # Solo el n√∫mero
                    })
                    print(f"‚úÖ A√±adido: {title[:30]}...")
            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando libro: {str(e)}")
                continue
                
        page += 1
        if page > 3:  # L√≠mite de p√°ginas para pruebas
            break
    
    df = pd.DataFrame(books[:num_books])
    print(f"üìä Total de libros obtenidos: {len(df)}")
    ti.xcom_push(key='book_data', value=df.to_dict('records'))

def insert_book_data_into_postgres(ti):
    book_data = ti.xcom_pull(key='book_data', task_ids='fetch_book_data')
    print(f"üì• Datos recibidos para insertar: {len(book_data) if book_data else 0} registros")
    
    if not book_data:
        raise ValueError("No se encontraron datos de libros")
    
    hook = PostgresHook(postgres_conn_id='books_connection')
    conn = hook.get_conn()
    cursor = conn.cursor()
    
    try:
        for book in book_data:
            cursor.execute("""
                INSERT INTO books (title, authors, price, rating)
                VALUES (%s, %s, %s, %s)
            """, (book['Title'], book['Author'], book['Price'], book['Rating']))
        conn.commit()
        print(f"üíæ Insertados {len(book_data)} registros en PostgreSQL")
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Error en la inserci√≥n: {str(e)}")
        raise
    finally:
        cursor.close()
        conn.close()

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 5, 14),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='fetch_and_store_amazon_books',
    default_args=default_args,
    description='A simple DAG to fetch book data from Amazon and store it in Postgres',
    schedule=timedelta(days=1),
)

#operators : Python Operator and PostgresOperator
#hooks - allows connection to postgres


fetch_book_data_task = PythonOperator(
    task_id='fetch_book_data',
    python_callable=get_amazon_data_books,
    op_args=[50],  # Number of books to fetch
    dag=dag,
)

create_table_task = SQLExecuteQueryOperator(
    task_id='create_table',
    conn_id='books_connection',
    sql="""
    CREATE TABLE IF NOT EXISTS books (
        id SERIAL PRIMARY KEY,
        title TEXT NOT NULL,
        authors TEXT,
        price TEXT,
        rating TEXT
    );
    """,
    dag=dag,
)

insert_book_data_task = PythonOperator(
    task_id='insert_book_data',
    python_callable=insert_book_data_into_postgres,
    dag=dag,
)

#dependencies

fetch_book_data_task >> create_table_task >> insert_book_data_task
            </div>
            <p>En la secci√≥n de DAGs, localiza y ejecuta el DAG denominado fetch_and_store_amazon_books haciendo clic en el bot√≥n de "Play".</p>
            <p>Posteriormente, dir√≠gete a pgAdmin para verificar los datos recopilados de Amazon. Para ello, haz clic derecho sobre la secci√≥n "Tables" y selecciona la opci√≥n "Query Tool".</p>
            <div class="blog-image-zoom">
                <img src="media/airflow19.png" alt="">
            </div>
            <h2>Proyecto en la Nube</h2>
            <p>En el mundo del an√°lisis de datos, la capacidad de extraer informaci√≥n valiosa de redes sociales como Twitter y almacenarla de manera eficiente es fundamental. Un proyecto ETL (Extract, Transform, Load) que utiliza Python para conectarse a la API de Twitter, procesar los datos y cargarlos en un bucket de Amazon S3 puede ser una soluci√≥n escalable y automatizada para este prop√≥sito.</p>
            <p>El proyecto consiste en un flujo automatizado que:</p>
            <ol>
                <li><marcador class="resaltado8">Extrae </marcador>tweets en tiempo real o hist√≥ricos mediante la API de Twitter (usando bibliotecas como tweepy o python-twitter).</li>
                <li><marcador class="resaltado8">Transforma </marcador>los datos crudos, aplicando filtros, limpieza de texto, an√°lisis de sentimientos o estructurado en un formato √≥ptimo (como CSV, JSON o Parquet).</li>
                <li><marcador class="resaltado8">Carga </marcador>la informaci√≥n procesada en un bucket de Amazon S3, utilizando el SDK de AWS (boto3), donde podr√° ser consultada por herramientas de analytics o machine learning.</li>
            </ol>
            <p>Este proceso puede ejecutarse peri√≥dicamente mediante servicios como AWS Lambda o Airflow, garantizando una base de datos actualizada para su posterior an√°lisis.</p>
            <h6>Generar un Access Token para Twitter</h6>
            <p>Para comenzar a usar la API de Twitter, dir√≠gete a <a href="https://developer.twitter.com" target="blank">Twitter Developer Platform</a>, inicia sesi√≥n con tu cuenta de Twitter, y luego solicita el acceso de desarrollador gratuito a trav√©s del nivel "Essential".</p>
            <p>Una vez dentro del panel de desarrollador de Twitter, navega a la secci√≥n "Projects & Apps" y selecciona "Overview". Aqu√≠, proceder√°s a usar el proyecto default. Este proyecto te proporcionar√° las credenciales esenciales para interactuar con la API de Twitter: el Bearer Token , con las opciones de generar el Bearer Token.</p>
            <h6>Conexi√≥n con la API de Twitter</h6>
            <div class="contenedor">
                <div class="etiqueta">
                    Creaci√≥n de un entorno virtual:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="python3 -m venv venv" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">
                    Activar el entorno virtual:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="source venv/bin/activate" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Instalar Dependencias:</div>
                <div class="comandos">
                    <input type="text" class="text" value="pip3 install pandas s3fs tweepy" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>‚å≠ Crear un script en Python que se conecta a la API de Twitter usando tweepy y recupera los 10 √∫ltimos tweets de un usuario, mostr√°ndolos por pantalla. Sirve para analizar o monitorear contenido p√∫blico de una cuenta.</p>
            <div class="archivo">
import tweepy
import time

# Bearer Token de tu proyecto (API v2)
bearer_token = ""  # Sustituye con tu Bearer Token

# Crear cliente con API v2
client = tweepy.Client(bearer_token=bearer_token)

# Nombre de usuario (sin el @)
username = "elonmusk"

# Obtener el ID del usuario a partir del nombre
try:
    user = client.get_user(username=username)
    user_id = user.data.id

    #Obtener los √∫ltimos tweets del usuario
    response = client.get_users_tweets(id=user_id, max_results=10, tweet_fields=["created_at", "text"])

    #Imprimir los tweets
    for tweet in response.data:
        print(tweet.text)


except tweepy.TooManyRequests as e:
    print("L√≠mite de peticiones alcanzado. Esperando 15 minutos...")
    time.sleep(15 * 60)  # Esperar 15 minutos
            </div>
            <p>‚å≠ El siguiente script se conecta con la API v2 de Twitter para extraer los √∫ltimos tweets del usuario especificado (en este caso, elonmusk), y luego guarda informaci√≥n relevante como el texto del tweet, n√∫mero de likes, retweets y la fecha de creaci√≥n en un archivo CSV llamado refined_tweets.csv.</p>
            <div class="archivo">
import tweepy
import pandas as pd
import json
from datetime import datetime
import s3fs  # Opcional si vas a guardar en S3, puedes omitir si solo guardas localmente

def run_twitter_etl():
    # Token de autenticaci√≥n (Bearer Token de API v2)
    bearer_token = ""  # Sustituye con tu Bearer Token

    # Cliente de Twitter API v2
    client = tweepy.Client(bearer_token=bearer_token)

    # Nombre de usuario sin el '@'
    username = "elonmusk"

    try:
        # Obtener informaci√≥n del usuario (necesario para obtener su ID)
        user_response = client.get_user(username=username)
        if user_response.data is None:
            print(f"Usuario '{username}' no encontrado.")
            return
        user = user_response.data
        user_id = user.id
        print(f"ID del usuario '{username}': {user_id}")

        # Obtener √∫ltimos tweets del usuario
        response = client.get_users_tweets(
            id=user_id,
            max_results=10,  # M√°ximo permitido por request en API gratuita
            tweet_fields=["created_at", "text"]
        )

        # Verificar si se recibieron tweets
        tweets_data = response.data
        if tweets_data is None:
            print("No se encontraron tweets o el acceso est√° limitado por el nivel de la cuenta.")
            return

        # Procesar los tweets recibidos
        tweets = []
        for tweet in tweets_data:
            refined_tweet = {
                "user": user.username,
                "text": tweet.text,
                "created_at": tweet.created_at
            }
            tweets.append(refined_tweet)

        # Crear DataFrame y mostrar su contenido
        df = pd.DataFrame(tweets)
        print("\nContenido del DataFrame:")
        print(df)

        # Guardar los datos en un archivo CSV
        df.to_csv('refined_tweets.csv', index=False)
        print("\nArchivo 'refined_tweets.csv' creado con √©xito.")

    except tweepy.TooManyRequests:
        print("Has excedido el l√≠mite de peticiones. Espera unos minutos antes de volver a intentar.")
    except tweepy.TweepyException as e:
        print("Error al conectar con la API de Twitter:", e)

# Ejecutar la funci√≥n
run_twitter_etl()
            </div>
            <div class="contenedor">
                <div class="etiqueta">Instalaci√≥n de Dependencias:</div>
                <div class="comandos">
                    <input type="text" class="text" value="pip install --upgrade google-api-python-client" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Como variante se puede utilizar la API de YouTube Data v3 para extraer y procesar todos los comentarios de un video espec√≠fico de YouTube. Primero se configura la conexi√≥n con la API, luego se realizan solicitudes paginadas para obtener los comentarios (hasta 100 por petici√≥n) y las procesa para extraer informaci√≥n clave como el autor, el texto del comentario y la fecha de publicaci√≥n. La aplicaci√≥n maneja autom√°ticamente la paginaci√≥n para recolectar todos los comentarios disponibles. Los datos son estructurados en una lista de diccionarios para su f√°cil an√°lisis, mostrando finalmente un resumen con el total de comentarios recolectados. El c√≥digo incluye manejo de errores para casos donde la estructura de los comentarios no sea la esperada, y est√° dise√±ado para ser f√°cilmente integrable en sistemas m√°s grandes de an√°lisis de datos o moderaci√≥n de contenido.</p>
            <p>Para obtener una Clave de API, ve a la <a href="https://console.developers.google.com/" target="blank">Consola de APIs de Google</a>, donde podr√°s crear un nuevo proyecto o seleccionar uno ya existente. Despu√©s, activa la API de YouTube Data API v3 y dir√≠gete a la secci√≥n "Credenciales" para crear una nueva Clave de API, la cual deber√°s copiar una vez generada.</p>
            <div class="blog-image-zoom">
                <img src="media/airflow28.png" alt="">
            </div>
            <p>‚å≠ El script para obtener comentarios de YouTube quedar√≠a de la siguiente forma:</p>
            <div class="archivo">
import os
import csv
import googleapiclient.discovery

def main():
    """
    Funci√≥n principal que obtiene comentarios de un video de YouTube usando la API v3.
    """
    # Configuraci√≥n inicial (solo para desarrollo)
    # Deshabilita la verificaci√≥n HTTPS de OAuthlib cuando se ejecuta localmente.
    # ¬°NO mantener esta opci√≥n activa en producci√≥n!
    os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"

    # Configuraci√≥n de la API de YouTube
    api_service_name = "youtube"
    api_version = "v3"
    DEVELOPER_KEY = "TU_CLAVE_DE_API"  # Reemplaza con tu clave real

    # ID del video de YouTube que quieres analizar
    video_id = "q8q3OFFfY6c"  # Ejemplo - reemplaza con tu video ID

    # Inicializa el cliente de la API de YouTube
    youtube = googleapiclient.discovery.build(
        api_service_name, 
        api_version, 
        developerKey=DEVELOPER_KEY
    )

    # Lista para almacenar todos los comentarios
    comments_list = []

    # Primera solicitud para obtener comentarios
    request = youtube.commentThreads().list(
        part="snippet,replies",
        videoId=video_id,
        maxResults=100  # M√°ximo permitido por la API
    )
    response = request.execute()

    # Procesa los comentarios iniciales
    comments_list.extend(process_comments(response['items']))

    # Paginaci√≥n: sigue solicitando comentarios mientras haya m√°s p√°ginas
    while response.get('nextPageToken', None):
        request = youtube.commentThreads().list(
            part='snippet,replies',
            videoId=video_id,
            pageToken=response['nextPageToken'],
            maxResults=100
        )
        response = request.execute()
        comments_list.extend(process_comments(response['items']))

    # Muestra resultados finales
    print(f"\nTotal de comentarios obtenidos: {len(comments_list)}")
    print("\nPrimeros 3 comentarios:")
    for comment in comments_list[:3]:
        print(f"Autor: {comment['author']}")
        print(f"Comentario: {comment['comment'][:50]}...")  # Muestra solo los primeros 50 caracteres
        print(f"Fecha: {comment['published_at']}")
        print("-" * 50)

    # Guardar en un archivo CSV
    with open("comentarios_youtube.csv", mode="w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["author", "comment", "published_at"])
        writer.writeheader()
        writer.writerows(comments_list)
    print("\n‚úÖ Comentarios guardados en 'comentarios_youtube.csv'")

def process_comments(response_items):
    """
    Procesa los comentarios crudos de la API y extrae informaci√≥n relevante.
    
    Args:
        response_items: Lista de comentarios sin procesar de la API
        
    Returns:
        Lista de diccionarios con informaci√≥n estructurada de cada comentario
    """
    comments = []
    for comment in response_items:
        try:
            top_comment = comment['snippet']['topLevelComment']['snippet']
            author = top_comment['authorDisplayName']
            comment_text = top_comment['textOriginal']
            publish_time = top_comment['publishedAt']
            
            comment_info = {
                'author': author,
                'comment': comment_text,
                'published_at': publish_time
            }
            comments.append(comment_info)
        except KeyError as e:
            print(f"Error procesando comentario: {e}")
    
    print(f'Procesados {len(comments)} comentarios en este lote.')
    return comments

if __name__ == "__main__":
    main()
            </div>
            <h6>Creaci√≥n de Recusos en AWS</h6>
            <p>Para desplegar Apache Airflow en una instancia EC2 de AWS, primero debes iniciar sesi√≥n en la consola de AWS y dirigirte al servicio EC2 para lanzar una nueva instancia. Elige una Amazon Machine Image (AMI) basada en Ubuntu, como Ubuntu Server 24.04 LTS, ya que es compatible y ampliamente usada. Luego, selecciona un tipo de instancia adecuado; para pruebas, una t2.micro es suficiente si est√°s en el plan gratuito. Configura el almacenamiento (el valor por defecto de 8 GB suele ser suficiente) y elige o crea un par de claves SSH para acceder de forma segura a tu m√°quina. Aseg√∫rate de configurar correctamente el grupo de seguridad: permitiendo el tr√°fico en el puerto 22 para el acceso SSH y el puerto 8080 para la interfaz web de Airflow.</p>
            <div class="blog-image-doble-grande">
                <img src="media/airflow20.png" alt="">
                <img src="media/airflow21.png" alt="">
            </div>
            <p>Para iniciar una sesi√≥n SSH en tu instancia EC2, primero necesitas ubicarte en el directorio donde se encuentran las claves SSH que descargaste previamente, ya que estas son esenciales para la autenticaci√≥n segura. Una vez en la carpeta correcta, dir√≠gete a la consola de AWS, selecciona tu instancia de Airflow en la secci√≥n EC2, y haz clic en el bot√≥n "Conectar"; all√≠ encontrar√°s los detalles y el comando SSH exacto que debes usar para acceder a tu m√°quina.</p>
            <div class="blog-image-zoom">
                <img src="media/airflow22.png" alt="">
            </div>
            <div class="contenedor">
                <div class="etiqueta">Cambiar los permisos para la clave ssh:</div>
                <div class="comandos">
                    <input type="text" class="text" value="chmod 400 airflow_ec2_key.pem" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Conectarse a la instancia:</div>
                <div class="comandos">
                    <input type="text" class="text" value="ssh -i 'airflow_ec2_key.pem' ubuntu@ec2-1-111-11-11.us-east-2.compute.amazonaws.com" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Actualizar las dependencias:</div>
                <div class="comandos">
                    <input type="text" class="text" value="sudo apt-get update" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Instalar Python3:</div>
                <div class="comandos">
                    <input type="text" class="text" value="sudo apt install python3-pip" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Instalar Airflow:</div>
                <div class="comandos">
                    <input type="text" class="text" value="sudo pip install apache-airflow --ignore-installed --break-system-packages" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Instalar Dependencias:</div>
                <div class="comandos">
                    <input type="text" class="text" value="sudo pip install pandas s3fs tweepy flask-appbuilder graphviz --ignore-installed --break-system-packages" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Buscar Airflow:</div>
                <div class="comandos">
                    <input type="text" class="text" value="find ~/.local/bin -name airflow" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Agregar Airflow al PATH:</div>
                <div class="comandos">
                    <input type="text" class="text" value="export PATH=&quot;$HOME/.local/bin:$PATH&quot;" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Crear la carpeta para almacenar los DAGs:</div>
                <div class="comandos">
                    <input type="text" class="text" value="mkdir twitter_dags" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Posteriormente, deber√°s editar el archivo de configuraci√≥n de Airflow, <marcador class="resaltado1">airflow.cfg</marcador>. Dentro de este archivo, busca la variable <marcador class="subrayado">dags_folder</marcador> y modifica su valor para que apunte a la carpeta que creaste para tus DAGs, espec√≠ficamente a la ruta <marcador class="resaltado1">/home/ubuntu/airflow/twitter_dag</marcador>.</p>
            <p>Usar el SequentialExecutor es la mejor opci√≥n para tu servidor con recursos limitados, ya que es m√°s simple y ligero. Para configurarlo, deber√°s abrir tu archivo <marcador class="resaltado1">airflow.cfg</marcador> y cambiar la configuraci√≥n de executor de LocalExecutor a SequentialExecutor.</p>
            <p>Adicionalmente, dentro del mismo archivo <marcador class="resaltado1">airflow.cfg</marcador>, localiza la secci√≥n [core] y ajusta los siguientes par√°metros: establece parallelism = 1, max_active_runs_per_dag = 1, y workers = 1.</p>
            <div class="contenedor">
                <div class="etiqueta">
                    Crear las carpetas para la gesti√≥n de Airflow:
                </div>
                <div class="comandos">
                    <input type="text" class="text" value="mkdir -p ./dags ./ logs ./plugins ./config" oninput="ajustarAncho(this)">
                    <button><i class="fa fa-clone"></i><span> Copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Inicializar Airflow y la base de datos:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow db migrate" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Crear el usuario admin::</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Si te encuentras con problemas para a√±adir un nuevo usuario mediante los comandos habituales, una alternativa es utilizar <marcador class="subrayado">airflow standalone</marcador>. Sin embargo, ten en cuenta que este modo inicia muchos servicios simult√°neamente, lo que podr√≠a requerir una instancia EC2 con m√°s memoria de la que ofrece el plan gratuito, incurriendo en costos. Puedes optar por iniciar <marcador class="subrayado">airflow standalone</marcador> solo el tiempo suficiente para crear el usuario, asegur√°ndote de guardar bien las credenciales de acceso. Es importante considerar que, debido a la posible saturaci√≥n de la instancia en este modo, podr√≠a ser necesario un reinicio manual desde la consola de EC2. Si esto sucede, tambi√©n necesitar√°s obtener una nueva conexi√≥n SSH, ya que la instancia podr√≠a cambiar su direcci√≥n, aunque tu clave SSH original seguir√° siendo v√°lida.</p>
            <div class="contenedor">
                <div class="etiqueta">Levantar solo el servidor web:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow api-server --port 8080 --workers 1" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Luego ejecuta el scheduler en otra terminal:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow scheduler" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Para acceder a la interfaz web de Airflow en tu instancia EC2, primero dir√≠gete a la secci√≥n de instancias en la consola de AWS y haz clic sobre tu instancia de Airflow para obtener su DNS p√∫blico. Luego, pega este DNS en tu navegador web, asegur√°ndote de apuntar al puerto 8080 (ej. http://tu-dns-publico:8080). Es crucial que la aplicaci√≥n sea accesible externamente; para ello, navega hasta la secci√≥n de "Seguridad" dentro de los detalles de tu instancia, entra en el grupo de seguridad asociado y, a trav√©s del bot√≥n "Editar reglas de entrada" (Edit inbound rules), a√±ade una nueva regla que permita todo el tr√°fico (All traffic) desde cualquier origen (Anywhere-IPv4).</p>
            <p>Ten en cuenta que permitir todo el tr√°fico desde cualquier origen es un riesgo de seguridad. Sin embargo, dado que esta instancia es solo para pruebas y se eliminar√° despu√©s de terminar para evitar el consumo de recursos gratuitos, es aceptable en este contexto.</p>
            <div class="blog-image-doble-grande">
                <img src="media/airflow23.png" alt="">
                <img src="media/airflow24.png" alt="">
            </div>
            <p>Ahora que Airflow est√° funcional, es necesario tener nuestro DAG listo. Este DAG, llamado twitter_dag.py, har√° una llamada a la funci√≥n run_twitter_etl que se encuentra dentro de nuestro script de Twitter, twitter_etl.py.</p>
            <p>‚å≠ El siguiente DAG corre diariamente y ejecuta una funci√≥n Python llamada run_twitter_etl, la cual conecta a la API de Twitter, extrae datos y los transforma o guarda en S3 de AWS.</p>
            <div class="archivo">
# Importa la clase timedelta para manejar intervalos de tiempo.
from datetime import timedelta, datetime

# Importa la clase principal DAG de Airflow para definir el flujo de trabajo.
from airflow import DAG

# Importa el operador para ejecutar funciones de Python dentro del DAG.
from airflow.providers.standard.operators.python import PythonOperator  # Aseg√∫rate de usar este import

# Importa la funci√≥n personalizada que realiza el proceso ETL desde un archivo externo.
from twitter_etl import run_twitter_etl

# Define argumentos por defecto que se aplican a todas las tareas del DAG.
default_args = {
    'owner': 'airflow',                         # Due√±o del DAG (informativo)
    'depends_on_past': False,                   # No depende de ejecuciones anteriores
    'start_date': datetime(2020, 11, 8),        # Fecha de inicio del DAG
    'email': ['airflow@example.com'],           # Correo de contacto (opcional)
    'email_on_failure': False,                  # No enviar correo si falla
    'email_on_retry': False,                    # No enviar correo al reintentar
    'retries': 1,                               # Reintentar una vez en caso de error
    'retry_delay': timedelta(minutes=1)         # Esperar 1 minuto antes del reintento
}

# Define el DAG principal sin 'schedule_interval' aqu√≠
dag = DAG(
    'twitter_dag',                              # Nombre del DAG
    default_args=default_args,                  # Usa los argumentos definidos antes
    description='Our first DAG with ETL process!',
    catchup=False                               # Para evitar que ejecute dag runs pasados
)

# Asigna 'schedule_interval' despu√©s de la creaci√≥n del DAG
dag.schedule_interval = timedelta(days=1)      # Se ejecuta cada 1 d√≠a

# Define una tarea del DAG que ejecuta la funci√≥n 'run_twitter_etl' desde el archivo importado.
run_etl = PythonOperator(
    task_id='complete_twitter_etl',             # Identificador de la tarea
    python_callable=run_twitter_etl,            # Funci√≥n que se ejecutar√°
    dag=dag,                                    # DAG al que pertenece la tarea
)

# Ejecuta la tarea (√∫til para visualizar en ciertas versiones, aunque no siempre necesario).
run_etl     
            </div>
            <p>Para crear un bucket S3, inicia sesi√≥n en la Consola de AWS y navega hasta el servicio S3. Una vez all√≠, haz clic en "Crear bucket" y as√≠gnale un nombre √∫nico que sea globalmente distintivo (por ejemplo, airflow-bucket-twitter-datos). Puedes dejar las configuraciones de bloqueo de acceso p√∫blico y el resto de las opciones por defecto para empezar, a menos que tengas requisitos de seguridad espec√≠ficos. Finalmente, haz clic en "Crear bucket" para completar el proceso.</p>
            <p>En el script ETL es necesario modificar la l√≠nea donde se guarda el DataFrame para que apunte al bucket de Amazon S3. Esto se logra reemplazando la ruta local por df.to_csv('s3://airflow-bucket-twitter-datos/refined_tweets.csv', index=False), lo cual permite que los datos procesados se almacenen directamente en el bucket especificado.</p>
            <p>En una terminal separada, necesitar√°s establecer una nueva conexi√≥n SSH a tu instancia EC2. Una vez que hayas accedido exitosamente a la instancia, el siguiente paso es navegar hasta el directorio de Airflow, que es donde se almacenan y gestionan los DAGs de tu proyecto.</p>
            <p>Ahora, dentro del directorio <marcador class="resaltado1">twitter_dags</marcador>, es fundamental crear los dos archivos de Python: <marcador class="resaltado1">twitter_etl.py</marcador> y <marcador class="resaltado1">twitter_dag.py</marcador>. Es crucial recordar que tu script <marcador class="resaltado1">twitter_etl.py</marcador>debe estar configurado para apuntar al bucket S3 que se crear√°, ya que all√≠ se almacenar√°n los datos procesados, tal como se mostrar√° m√°s adelante.</p>
            <p>Despu√©s de realizar esos cambios, es necesario detener y reiniciar los servicios de Airflow para que detecte las nuevas configuraciones. Y luego, vuelve a ejecutar los comandos <marcador class="subrayado">airflow api-server --port 8080 --workers 1</marcador> y <marcador class="subrayado">airflow scheduler</marcador> para reiniciar el servicio de Airflow. Esto asegurar√° que los cambios en la carpeta de DAGs y la configuraci√≥n sean aplicados correctamente.</p>
            <div class="contenedor">
                <div class="etiqueta">Encuentra el PID del proceso que est√° usando el puerto con:</div>
                <div class="comandos">
                    <input type="text" class="text" value="lsof -i :8793" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Aseg√∫rate de detener cualquier proceso que est√© ocupando el puerto antes de iniciar el scheduler.</p>
            <div class="contenedor">
                <div class="etiqueta">Matar los procesos que est√°n ocupando el puerto:</div>
                <div class="comandos">
                    <input type="text" class="text" value="kill -9 1218 1291 1292" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Matar los procesos asociados a Airflow:</div>
                <div class="comandos">
                    <input type="text" class="text" value="pkill -f airflow" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Matar los procesos asociados a Airflow:</div>
                <div class="comandos">
                    <input type="text" class="text" value="pkill -f &quot;airflow scheduler&quot;" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Matar los procesos asociados a gunicorn:</div>
                <div class="comandos">
                    <input type="text" class="text" value="pkill -f gunicorn" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Eliminar archivos PID residuales:</div>
                <div class="comandos">
                    <input type="text" class="text" value="rm -f ~/airflow/*.pid && rm -f ~/airflow/webserver_config.py" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Permisos extendidos:</div>
                <div class="comandos">
                    <input type="text" class="text" value="sudo chmod -R 755 ~/airflow/twitter_dag" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Si necesitas resetear completamente la base de datos, puedes usar:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow db reset" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Cuidado: reset elimina todo lo que tengas en la base de datos de Airflow (DAGs ejecutados, conexiones, variables, etc.).</p>
            <div class="contenedor">
                <div class="etiqueta">Aplicar todas las migraciones necesarias a la base de datos:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow db migrate" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Levantar el servidor web:</div>
                <div class="comandos">
                    <input type="text" class="text" value="AIRFLOW__LOGGING__LOGGING_LEVEL=DEBUG airflow api-server --port 8080 --workers 1" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Luego ejecuta el scheduler en otra terminal:</div>
                <div class="comandos">
                    <input type="text" class="text" value="AIRFLOW__LOGGING__LOGGING_LEVEL=DEBUG airflow scheduler" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Verificar que el DAG est√© disponible en la lista:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow dags list" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="contenedor">
                <div class="etiqueta">Ejecutar el DAG manualmente:</div>
                <div class="comandos">
                    <input type="text" class="text" value="airflow dags trigger twitter_dag" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <p>Para otorgar permisos de escritura a tu bucket S3, ve a la secci√≥n de Instancias en la consola de AWS. Arriba, en el men√∫ desplegable de "Acciones" (Actions), selecciona "Seguridad" (Security) y luego "Modificar rol de IAM" (Modify IAM role). Aqu√≠, crea un nuevo rol de IAM, asign√°ndolo al servicio EC2 para permitirle interactuar con otros servicios de AWS. A continuaci√≥n, busca y adjunta las pol√≠ticas AmazonS3FullAccess y AmazonEC2FullAccess (esta √∫ltima es opcional para el contexto de S3, pero √∫til para gesti√≥n de EC2). Finalmente, nombra el rol ec2_s3_airflow_role. Tras refrescar la p√°gina, el nuevo rol deber√≠a aparecer y estar listo para ser asociado a tu instancia.</p>
            <div class="blog-image-doble-grande">
                <img src="media/airflow25.jpg" alt="">
                <img src="media/airflow26.png" alt="">
            </div>
            <p>Fue imposible hacer que los DAGs corrieran en la interfaz web usando la instancia gratuita de AWS EC2. Aunque se intent√≥ especificar y reiniciar rutas alternativas en el archivo de configuraci√≥n, Airflow simplemente no detectaba los DAGs al iniciar el scheduler y la interfaz web por separado. El modo standalone de Airflow, que podr√≠a haber sido una soluci√≥n, tampoco funcion√≥ debido a la limitaci√≥n de recursos de la instancia gratuita. Sin embargo, al ejecutar todo el proceso de forma manual, el flujo completo se complet√≥ sin errores, lo que sugiere que el problema reside en la capacidad de la instancia para gestionar simult√°neamente todos los servicios de Airflow necesarios para la detecci√≥n autom√°tica de DAGs.</p>
            <p>En una instancia con m√°s recursos, el modo standalone de Airflow deber√≠a ejecutarse sin ning√∫n problema.</p>
            <div class="contenedor">
                <div class="etiqueta">Ejecutar el DAG manualmente con python:</div>
                <div class="comandos">
                    <input type="text" class="text" value="python3 twitter_dag.py" oninput="ajustarancho(this)">
                    <button><i class="fa fa-clone"></i><span> copy</span></button>
                </div>
            </div>
            <div class="blog-image-zoom">
                <img src="media/airflow27.png" alt="">
            </div>
            </div>
        </div>       
    </main>
    <script src="../js/script.js"></script>
</body>
</html>